#!/usr/bin/env python3

import json
import os
import re
import csv
from collections import Counter, defaultdict
from pathlib import Path
import argparse

def load_jsonl(file_path):
    """åŠ è½½JSONLæ–‡ä»¶"""
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path}")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")
        return None
    return data

def get_task_type(file_path):
    """æ ¹æ®æ–‡ä»¶è·¯å¾„åˆ¤æ–­ä»»åŠ¡ç±»å‹"""
    if "task1" in file_path or "task3" in file_path:
        return "binary_classification"  # task1å’Œtask3éƒ½æ˜¯äºŒåˆ†ç±»
    elif "task2" in file_path:
        return "binary_classification"  # task2å®é™…ä¸Šä¹Ÿæ˜¯äºŒåˆ†ç±»
    else:
        return "unknown"

def get_task2_subclass(file_path):
    """è·å–task2çš„å­ç±»ä¿¡æ¯"""
    # åªä»æ–‡ä»¶åä¸­æå–å­ç±»ä¿¡æ¯ï¼Œé¿å…ç›®å½•åå¹²æ‰°
    import os
    filename = os.path.basename(file_path)
    
    if "style_based" in filename:
        if "legitimate" in filename:
            return "style_based_legitimate"
        else:
            return "style_based_fake"
    elif "content_based" in filename:
        return "content_based_fake"
    elif "integration_based" in filename:
        if "legitimate" in filename:
            return "integration_based_legitimate"
        else:
            return "integration_based_fake"
    elif "story_based" in filename:
        return "story_based_fake"
    return "unknown"

def normalize_prediction(pred_text, task_type, task2_subclass=None):
    """
    æ ‡å‡†åŒ–é¢„æµ‹æ–‡æœ¬ä¸ºç»Ÿä¸€æ ¼å¼
    """
    if not pred_text:
        return "unknown"
    
    # æ›´å½»åº•åœ°æ¸…ç†é¢„æµ‹æ–‡æœ¬
    pred_clean = str(pred_text).strip()
    # ç§»é™¤æ‰€æœ‰æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    pred_clean = re.sub(r'[\n\r\t]+', ' ', pred_clean)
    pred_clean = re.sub(r'\s+', ' ', pred_clean)
    pred_clean = pred_clean.strip().lower()
    
    if task_type == "binary_classification":
        # å¯¹äºTask2ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if task2_subclass:
            # ä¼˜å…ˆæ£€æŸ¥ä¸ç¡®å®šåŒ¹é…
            uncertain_phrases = [
                'cannot determine', 'unable to determine', 'not sure', 'uncertain', 'unclear',
                'hard to tell', 'difficult to determine', 'cannot tell', 'ambiguous',
                'æ— æ³•ç¡®å®š', 'ä¸èƒ½ç¡®å®š', 'æ— æ³•åˆ¤æ–­', 'ä¸èƒ½åˆ¤æ–­', 'ä¸ç¡®å®š', 'ä¸æ¸…æ¥š', 'éš¾ä»¥åˆ¤æ–­'
            ]
            if any(phrase in pred_clean for phrase in uncertain_phrases):
                return "uncertain"
            
            if "fake" in task2_subclass:
                # å¯¹äºfakeå­ç±»ï¼šæ£€æŸ¥ç›®æ ‡ç±»åˆ« vs other
                if task2_subclass == "style_based_fake":
                    style_keywords = ['style-based', 'style based', 'stylistic', 'style', 'é£æ ¼']
                    if any(keyword in pred_clean for keyword in style_keywords):
                        return "legitimate"  # æ­£ç±»
                elif task2_subclass == "content_based_fake":
                    content_keywords = ['content-based', 'content based', 'content', 'å†…å®¹']
                    if any(keyword in pred_clean for keyword in content_keywords):
                        return "legitimate"  # æ­£ç±»
                elif task2_subclass == "integration_based_fake":
                    integration_keywords = ['integration-based', 'integration based', 'integration', 'æ•´åˆ']
                    if any(keyword in pred_clean for keyword in integration_keywords):
                        return "legitimate"  # æ­£ç±»
                elif task2_subclass == "story_based_fake":
                    story_keywords = ['story-based', 'story based', 'story', 'æ•…äº‹']
                    if any(keyword in pred_clean for keyword in story_keywords):
                        return "legitimate"  # æ­£ç±»
                
                # æ£€æŸ¥otherå…³é”®è¯
                other_keywords = ['other', 'others', 'different', 'å…¶ä»–']
                if any(keyword in pred_clean for keyword in other_keywords):
                    return "fake"  # è´Ÿç±»
                    
            elif "legitimate" in task2_subclass:
                # å¯¹äºlegitimateå­ç±»ï¼šstyle-based vs integration-based
                style_keywords = ['style-based', 'style based', 'stylistic', 'style', 'é£æ ¼']
                integration_keywords = ['integration-based', 'integration based', 'integration', 'æ•´åˆ']
                
                if any(keyword in pred_clean for keyword in style_keywords):
                    return "legitimate"  # style-based
                elif any(keyword in pred_clean for keyword in integration_keywords):
                    return "fake"  # integration-based
        
        # Task1å’ŒTask3çš„ä¼ ç»ŸäºŒåˆ†ç±»é€»è¾‘
        else:
            # ä¼˜å…ˆæ£€æŸ¥ä¸ç¡®å®šåŒ¹é…ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
            uncertain_phrases = [
                # è‹±æ–‡çŸ­è¯­
                'cannot determine', 'unable to determine', 'not sure', 'uncertain', 'unclear',
                'hard to tell', 'difficult to determine', 'cannot tell', 'ambiguous',
                # ä¸­æ–‡çŸ­è¯­
                'æ— æ³•ç¡®å®š', 'ä¸èƒ½ç¡®å®š', 'æ— æ³•åˆ¤æ–­', 'ä¸èƒ½åˆ¤æ–­', 'ä¸ç¡®å®š', 'ä¸æ¸…æ¥š', 'éš¾ä»¥åˆ¤æ–­'
            ]
            if any(phrase in pred_clean for phrase in uncertain_phrases):
                return "uncertain"
            
            # ç„¶åæ£€æŸ¥æ­£å‘åŒ¹é… - è¡¨ç¤ºlegitimate/trueçš„è¯æ±‡ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
            legitimate_keywords = [
                # è‹±æ–‡å…³é”®è¯
                'legitimate', 'true', 'real', 'valid', 'genuine', 'authentic', 'correct', 'accurate', 'factual',
                # ä¸­æ–‡å…³é”®è¯
                'çœŸå®', 'çœŸ', 'åˆæ³•', 'æ­£ç¡®', 'å‡†ç¡®', 'å¯ä¿¡', 'å¯é ', 'çœŸçš„', 'æ˜¯çœŸçš„', 'ä¸æ˜¯å‡çš„', 'ç¡®å®'
            ]
            if any(word in pred_clean for word in legitimate_keywords):
                return "legitimate"
            
            # æœ€åæ£€æŸ¥è´Ÿå‘åŒ¹é… - è¡¨ç¤ºfake/falseçš„è¯æ±‡ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
            fake_keywords = [
                # è‹±æ–‡å…³é”®è¯
                'fake', 'false', 'unreal', 'invalid', 'misleading', 'deceptive', 'incorrect', 'inaccurate',
                # ä¸­æ–‡å…³é”®è¯
                'å‡', 'è™šå‡', 'é”™è¯¯', 'ä¸çœŸå®', 'ä¸å‡†ç¡®', 'ä¼ªé€ ', 'ç¼–é€ ', 'è¯¯å¯¼', 'å‡çš„', 'æ˜¯å‡çš„', 'ä¸çœŸ'
            ]
            if any(word in pred_clean for word in fake_keywords):
                return "fake"
                
    elif task_type == "multiclass_classification":
        # Task2çš„å¤šåˆ†ç±»é€»è¾‘ - ä¼˜å…ˆçº§ï¼šä¸ç¡®å®š > Other > å…·ä½“ç±»å‹
        
        # é¦–å…ˆæ£€æŸ¥ä¸ç¡®å®šçš„è¡¨è¿°
        uncertain_phrases = [
            'cannot determine', 'unable to determine', 'not sure', 'uncertain', 'unclear',
            'hard to tell', 'difficult to determine', 'cannot tell', 'ambiguous',
            'æ— æ³•ç¡®å®š', 'ä¸èƒ½ç¡®å®š', 'æ— æ³•åˆ¤æ–­', 'ä¸èƒ½åˆ¤æ–­', 'ä¸ç¡®å®š', 'ä¸æ¸…æ¥š', 'éš¾ä»¥åˆ¤æ–­'
        ]
        if any(phrase in pred_clean for phrase in uncertain_phrases):
            return "unknown"
        
        # ç„¶åæ£€æŸ¥Otherå…³é”®è¯ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
        other_keywords = [
            'other', 'others', 'different', 'else', 'another', 'other type', 'other types',
            'å…¶ä»–', 'åˆ«çš„', 'å…¶å®ƒ', 'å¦ä¸€ç§', 'ä¸åŒ'
        ]
        if any(keyword in pred_clean for keyword in other_keywords):
            return "other"
        
        # æ¥ç€æ£€æŸ¥å…·ä½“çš„ç±»å‹å…³é”®è¯
        
        # Style-Based å…³é”®è¯ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
        style_keywords = [
            'style-based', 'style based', 'stylistic', 'style',
            'é£æ ¼', 'æ ·å¼', 'æ–‡ä½“', 'é£æ ¼åŒ–', 'æ ·å¼åŒ–', 'æ–‡é£'
        ]
        if any(keyword in pred_clean for keyword in style_keywords):
            return "style-based"
        
        # Content-Based å…³é”®è¯ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
        content_keywords = [
            'content-based', 'content based', 'content', 'manipulated', 'modified',
            'å†…å®¹', 'å†…å®¹åŒ–', 'æ“æ§', 'ä¿®æ”¹', 'ç¯¡æ”¹', 'æ›´æ”¹'
        ]
        if any(keyword in pred_clean for keyword in content_keywords):
            return "content-based"
        
        # Integration-Based å…³é”®è¯ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
        integration_keywords = [
            'integration-based', 'integration based', 'integration', 'integrated', 'combined', 'merged',
            'æ•´åˆ', 'é›†æˆ', 'ç»¼åˆ', 'åˆå¹¶', 'èåˆ', 'ç»“åˆ'
        ]
        if any(keyword in pred_clean for keyword in integration_keywords):
            return "integration-based"
        
        # Story-Based å…³é”®è¯ï¼ˆè‹±æ–‡+ä¸­æ–‡ï¼‰
        story_keywords = [
            'story-based', 'story based', 'story', 'narrative', 'generated', 'created',
            'æ•…äº‹', 'å™è¿°', 'ç”Ÿæˆ', 'åˆ›é€ ', 'ç¼–å†™', 'è™šæ„'
        ]
        if any(keyword in pred_clean for keyword in story_keywords):
            return "story-based"
        
        # æœ€åï¼Œå¯¹äºfakeå­ç±»ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®çš„ç±»å‹ï¼Œä½†æœ‰fakeç›¸å…³è¯æ±‡ï¼Œå½’ä¸ºother
        if task2_subclass and 'fake' in task2_subclass:
            general_fake_keywords = [
                'fake', 'false', 'misleading',
                'å‡', 'è™šå‡', 'è¯¯å¯¼'
            ]
            if any(word in pred_clean for word in general_fake_keywords):
                return "other"
    
    return "unknown"

def calculate_metrics(tp, fp, fn):
    """è®¡ç®—Precisionã€Recallå’ŒF1-Score"""
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    return precision, recall, f1

def calculate_binary_classification_metrics(data, task_type, task2_subclass=None):
    """è®¡ç®—äºŒåˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†æŒ‡æ ‡"""
    if not data:
        return {}
    
    # ç»Ÿè®¡çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
    true_labels = []
    pred_labels = []
    
    for item in data:
        if 'predict' not in item or 'label' not in item:
            continue
            
        # æ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        pred_normalized = normalize_prediction(item['predict'], task_type, task2_subclass)
        
        # è·å–çœŸå®æ ‡ç­¾
        true_label = str(item['label']).strip()
        
        if task_type == "binary_classification":
            # å¯¹äºTask1å’ŒTask3çš„ä¼ ç»ŸäºŒåˆ†ç±»
            if task2_subclass is None:
                if true_label.lower() in ['fake', '0']:
                    true_labels.append("fake")
                elif true_label.lower() in ['legitimate', '1']:
                    true_labels.append("legitimate")
                else:
                    continue  # è·³è¿‡æœªçŸ¥æ ‡ç­¾
            # å¯¹äºTask2çš„äºŒåˆ†ç±»
            else:
                # ç›´æ¥ä½¿ç”¨æ ‡ç­¾ï¼Œå¹¶æ˜ å°„åˆ°legitimate/fake
                true_label_clean = true_label.lower()
                if "fake" in task2_subclass:
                    # å¯¹äºfakeå­ç±»ï¼šç›®æ ‡ç±»åˆ« vs other
                    if task2_subclass == "style_based_fake" and true_label_clean == "style-based":
                        true_labels.append("legitimate")  # è¿™é‡Œlegitimateè¡¨ç¤ºæ­£ç±»
                    elif task2_subclass == "content_based_fake" and true_label_clean == "content-based":
                        true_labels.append("legitimate")
                    elif task2_subclass == "integration_based_fake" and true_label_clean == "integration-based":
                        true_labels.append("legitimate")
                    elif task2_subclass == "story_based_fake" and true_label_clean == "story-based":
                        true_labels.append("legitimate")
                    elif true_label_clean == "other":
                        true_labels.append("fake")  # è¿™é‡Œfakeè¡¨ç¤ºè´Ÿç±»
                    else:
                        continue  # è·³è¿‡æœªçŸ¥æ ‡ç­¾
                elif "legitimate" in task2_subclass:
                    # å¯¹äºlegitimateå­ç±»ï¼šstyle-based vs integration-based
                    if true_label_clean == "style-based":
                        true_labels.append("legitimate")
                    elif true_label_clean == "integration-based":
                        true_labels.append("fake")
                    else:
                        continue  # è·³è¿‡æœªçŸ¥æ ‡ç­¾
                        
            pred_labels.append(pred_normalized)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp_legitimate = sum(1 for t, p in zip(true_labels, pred_labels) if t == "legitimate" and p == "legitimate")
    fp_legitimate = sum(1 for t, p in zip(true_labels, pred_labels) if t == "fake" and p == "legitimate")
    fn_legitimate = sum(1 for t, p in zip(true_labels, pred_labels) if t == "legitimate" and p != "legitimate")
    
    tp_fake = sum(1 for t, p in zip(true_labels, pred_labels) if t == "fake" and p == "fake")
    fp_fake = sum(1 for t, p in zip(true_labels, pred_labels) if t == "legitimate" and p == "fake")
    fn_fake = sum(1 for t, p in zip(true_labels, pred_labels) if t == "fake" and p != "fake")
    
    # è®¡ç®—æŒ‡æ ‡
    precision_legitimate, recall_legitimate, f1_legitimate = calculate_metrics(tp_legitimate, fp_legitimate, fn_legitimate)
    precision_fake, recall_fake, f1_fake = calculate_metrics(tp_fake, fp_fake, fn_fake)
    
    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    correct = tp_legitimate + tp_fake
    total = len(true_labels)
    accuracy = correct / total if total > 0 else 0.0
    
    return {
        'accuracy': accuracy,
        'total_samples': total,
        'correct_predictions': correct,
        'legitimate_metrics': {
            'precision': precision_legitimate,
            'recall': recall_legitimate,
            'f1_score': f1_legitimate,
            'tp': tp_legitimate,
            'fp': fp_legitimate,
            'fn': fn_legitimate
        },
        'fake_metrics': {
            'precision': precision_fake,
            'recall': recall_fake,
            'f1_score': f1_fake,
            'tp': tp_fake,
            'fp': fp_fake,
            'fn': fn_fake
        }
    }

def calculate_multiclass_metrics(data, task2_subclass):
    """è®¡ç®—å¤šåˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†æŒ‡æ ‡"""
    if not data:
        return {}
    
    # ç»Ÿè®¡çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
    true_labels = []
    pred_labels = []
    
    for item in data:
        if 'predict' not in item or 'label' not in item:
            continue
            
        # æ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        pred_normalized = normalize_prediction(item['predict'], "multiclass_classification", task2_subclass)
        
        # è·å–çœŸå®æ ‡ç­¾å¹¶æ¸…ç†æ ¼å¼
        true_label = str(item['label']).strip().lower()
        
        # ç›´æ¥ä½¿ç”¨æ ‡ç­¾ï¼Œè¿™æ˜¯å·²ç»æ ‡å‡†åŒ–åçš„æ•°æ®
        true_labels.append(true_label)
                
        pred_labels.append(pred_normalized)
    
    # è·å–æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«
    all_classes = list(set(true_labels + pred_labels))
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    class_metrics = {}
    total_correct = 0
    
    for class_name in all_classes:
        tp = sum(1 for t, p in zip(true_labels, pred_labels) if t == class_name and p == class_name)
        fp = sum(1 for t, p in zip(true_labels, pred_labels) if t != class_name and p == class_name)
        fn = sum(1 for t, p in zip(true_labels, pred_labels) if t == class_name and p != class_name)
        
        precision, recall, f1 = calculate_metrics(tp, fp, fn)
        
        class_metrics[class_name] = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'tp': tp,
            'fp': fp,
            'fn': fn
        }
        
        total_correct += tp
    
    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    total = len(true_labels)
    accuracy = total_correct / total if total > 0 else 0.0
    
    return {
        'accuracy': accuracy,
        'total_samples': total,
        'correct_predictions': total_correct,
        'class_metrics': class_metrics
    }

def get_model_name_from_path(file_path):
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ¨¡å‹åç§°"""
    # ä»æ–‡ä»¶åä¸­æå–æ¨¡å‹åç§°
    filename = os.path.basename(file_path)
    
    # æ³¨æ„ï¼šé¡ºåºå¾ˆé‡è¦ï¼å…ˆæ£€æŸ¥æ›´å…·ä½“çš„æ¨¡å‹åç§°
    if "Meta-Llama" in filename:
        return "LLaMA3-8B"
    elif "Qwen" in filename:
        return "Qwen1.5-7B"
    elif "Baichuan" in filename:
        return "Baichuan2-7B"
    elif "Mistral" in filename:
        return "Mistral-7B"
    elif "chatglm3-6b" in filename.lower():
        return "ChatGLM3-6B"
    elif "Llama" in filename or "LLaMA" in filename:
        return "LLaMA3-8B"
    else:
        return "Unknown"

def get_dataset_from_path(file_path):
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ•°æ®é›†ä¿¡æ¯ï¼ˆè®­ç»ƒå’Œæ¨ç†ä½¿ç”¨åŒä¸€ä¸ªæ•°æ®é›†ï¼‰"""
    filename = os.path.basename(file_path)
    
    # Task1å’ŒTask2çš„æ•°æ®é›†è¯†åˆ«
    if "_glm_" in filename:
        return "ChatGLM Dataset"
    elif "_llama3_" in filename or "_llama_" in filename:
        return "LLaMA Dataset"
    # Task3ä½¿ç”¨çš„æ˜¯ç‰¹å®šæ•°æ®é›†
    elif "_gossip_" in filename:
        return "GossipCop Dataset"
    elif "_polifact_" in filename:
        return "PolitiFact Dataset"
    else:
        return "Unknown"

def analyze_predictions(file_path):
    """åˆ†æå•ä¸ªæ–‡ä»¶çš„é¢„æµ‹ç»“æœ"""
    print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {file_path}")
    
    data = load_jsonl(file_path)
    if data is None:
        return None
    
    # åˆ¤æ–­ä»»åŠ¡ç±»å‹
    task_type = get_task_type(file_path)
    task2_subclass = get_task2_subclass(file_path) if "task2" in file_path else None
    
    print(f"   ä»»åŠ¡ç±»å‹: {task_type}")
    if task2_subclass:
        print(f"   Task2å­ç±»: {task2_subclass}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(data)}")
    
    # ç»Ÿè®¡åŸå§‹é¢„æµ‹å€¼
    predictions = []
    labels = []
    
    for item in data:
        if 'predict' in item:
            predictions.append(item['predict'])
        if 'label' in item:
            labels.append(item['label'])
    
    predict_counter = Counter(predictions)
    label_counter = Counter(labels)
    
    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    if task_type == "binary_classification":
        metrics = calculate_binary_classification_metrics(data, task_type, task2_subclass)
    else:
        metrics = {}
    
    result = {
        'file_path': file_path,
        'model_name': get_model_name_from_path(file_path),
        'dataset': get_dataset_from_path(file_path),
        'task_type': task_type,
        'task2_subclass': task2_subclass,
        'total_samples': len(data),
        'predict_values': dict(predict_counter),
        'label_values': dict(label_counter),
        'unique_predictions': list(predict_counter.keys()),
        'unique_labels': list(label_counter.keys()),
        'metrics': metrics
    }
    
    return result

def export_to_csv(analysis_results, output_file="prediction_analysis_results.csv"):
    """å¯¼å‡ºç»“æœåˆ°CSVæ–‡ä»¶"""
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
    task1_results = []
    task2_results = []
    task3_results = []
    
    for result in analysis_results:
        if result and 'metrics' in result:
            file_path = result['file_path']
            if 'task1' in file_path:
                task1_results.append(result)
            elif 'task2' in file_path:
                task2_results.append(result)
            elif 'task3' in file_path:
                task3_results.append(result)
    
    # åˆ›å»ºä¸‰ä¸ªCSVæ–‡ä»¶
    base_name = output_file.replace('.csv', '')
    
    # Task1 CSV
    if task1_results:
        task1_csv = f"{base_name}_Task1.csv"
        export_binary_task_csv(task1_results, task1_csv, "Task1")
        print(f"ğŸ“„ Task1 ç»“æœå·²å¯¼å‡ºåˆ°: {task1_csv}")
    
    # Task2 CSV - æŒ‰å­ç±»åˆ†åˆ«å¯¼å‡º
    if task2_results:
        # æŒ‰å­ç±»åˆ†ç»„
        fake_results = []
        legitimate_results = []
        
        for result in task2_results:
            if "fake" in result['task2_subclass']:
                fake_results.append(result)
            else:
                legitimate_results.append(result)
        
        # å¯¼å‡ºfakeå­ç±»
        if fake_results:
            task2_fake_csv = f"{base_name}_Task2_Fake.csv"
            export_task2_fake_csv(fake_results, task2_fake_csv)
            print(f"ğŸ“„ Task2 Fake ç»“æœå·²å¯¼å‡ºåˆ°: {task2_fake_csv}")
        
        # å¯¼å‡ºlegitimateå­ç±»
        if legitimate_results:
            task2_legitimate_csv = f"{base_name}_Task2_Legitimate.csv"
            export_task2_legitimate_csv(legitimate_results, task2_legitimate_csv)
            print(f"ğŸ“„ Task2 Legitimate ç»“æœå·²å¯¼å‡ºåˆ°: {task2_legitimate_csv}")
    
    # Task3 CSV
    if task3_results:
        task3_csv = f"{base_name}_Task3.csv"
        export_binary_task_csv(task3_results, task3_csv, "Task3")
        print(f"ğŸ“„ Task3 ç»“æœå·²å¯¼å‡ºåˆ°: {task3_csv}")

def export_binary_task_csv(results, output_file, task_name):
    """å¯¼å‡ºäºŒåˆ†ç±»ä»»åŠ¡çš„CSV"""
    csv_data = []
    
    for result in results:
        metrics = result['metrics']
        model_name = result['model_name']
        dataset = result['dataset']
        
        csv_row = {
            'Model': model_name,
            'Dataset': dataset,
            'Task': task_name,
            'Accuracy': f"{metrics['accuracy']:.4f}",
            'Legitimate_Precision': f"{metrics['legitimate_metrics']['precision']:.4f}",
            'Legitimate_Recall': f"{metrics['legitimate_metrics']['recall']:.4f}",
            'Legitimate_F1_Score': f"{metrics['legitimate_metrics']['f1_score']:.4f}",
            'Fake_Precision': f"{metrics['fake_metrics']['precision']:.4f}",
            'Fake_Recall': f"{metrics['fake_metrics']['recall']:.4f}",
            'Fake_F1_Score': f"{metrics['fake_metrics']['f1_score']:.4f}",
        }
        csv_data.append(csv_row)
    
    if csv_data:
        fieldnames = csv_data[0].keys()
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)

def export_task2_fake_csv(results, output_file):
    """å¯¼å‡ºTask2 Fakeå­ç±»çš„CSV"""
    csv_data = []
    
    for result in results:
        metrics = result['metrics']
        model_name = result['model_name']
        dataset = result['dataset']
        subclass = result['task2_subclass']
        
        legitimate_metrics = metrics['legitimate_metrics']
        fake_metrics = metrics['fake_metrics']
        
        # å¯¹äºfakeå­ç±»ï¼šç›®æ ‡ç±»åˆ« vs Other
        target_class = subclass.replace('_fake', '').replace('_', '-').title()
        csv_row = {
            'Model': model_name,
            'Dataset': dataset,
            'Subclass': target_class,
            'Accuracy': f"{metrics['accuracy']:.4f}",
            'Target_Class_Precision': f"{legitimate_metrics['precision']:.4f}",
            'Target_Class_Recall': f"{legitimate_metrics['recall']:.4f}",
            'Target_Class_F1_Score': f"{legitimate_metrics['f1_score']:.4f}",
            'Other_Precision': f"{fake_metrics['precision']:.4f}",
            'Other_Recall': f"{fake_metrics['recall']:.4f}",
            'Other_F1_Score': f"{fake_metrics['f1_score']:.4f}",
        }
        csv_data.append(csv_row)
    
    if csv_data:
        fieldnames = csv_data[0].keys()
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)

def export_task2_legitimate_csv(results, output_file):
    """å¯¼å‡ºTask2 Legitimateå­ç±»çš„CSV"""
    csv_data = []
    
    for result in results:
        metrics = result['metrics']
        model_name = result['model_name']
        dataset = result['dataset']
        subclass = result['task2_subclass']
        
        legitimate_metrics = metrics['legitimate_metrics']
        fake_metrics = metrics['fake_metrics']
        
        # å¯¹äºlegitimateå­ç±»ï¼šStyle-based vs Integration-based
        csv_row = {
            'Model': model_name,
            'Dataset': dataset,
            'Subclass': subclass,
            'Accuracy': f"{metrics['accuracy']:.4f}",
            'Style_Based_Precision': f"{legitimate_metrics['precision']:.4f}",
            'Style_Based_Recall': f"{legitimate_metrics['recall']:.4f}",
            'Style_Based_F1_Score': f"{legitimate_metrics['f1_score']:.4f}",
            'Integration_Based_Precision': f"{fake_metrics['precision']:.4f}",
            'Integration_Based_Recall': f"{fake_metrics['recall']:.4f}",
            'Integration_Based_F1_Score': f"{fake_metrics['f1_score']:.4f}",
        }
        csv_data.append(csv_row)
    
    if csv_data:
        fieldnames = csv_data[0].keys()
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)

def find_result_files(base_dir="megafakeTasks"):
    """æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶"""
    result_files = []
    
    if not os.path.exists(base_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return result_files
    
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.jsonl'):
                result_files.append(os.path.join(root, file))
    
    return sorted(result_files)

def print_prediction_analysis(analysis_results):
    """æ‰“å°é¢„æµ‹åˆ†æç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ¯ æ¨¡å‹é¢„æµ‹ç»“æœç»Ÿè®¡åˆ†æ")
    print("="*80)
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
    task1_results = []
    task2_results = []
    task3_results = []
    
    for result in analysis_results:
        if result and 'metrics' in result:
            file_path = result['file_path']
            if 'task1' in file_path:
                task1_results.append(result)
            elif 'task2' in file_path:
                task2_results.append(result)
            elif 'task3' in file_path:
                task3_results.append(result)
    
    # æ‰“å°Task1ç»“æœ
    if task1_results:
        print(f"\nğŸ“ˆ Task1 (äºŒåˆ†ç±»ä»»åŠ¡) - {len(task1_results)} ä¸ªæ–‡ä»¶:")
        total_accuracy = 0
        
        for result in task1_results:
            metrics = result['metrics']
            print(f"\nğŸ“ {result['file_path']}")
            print(f"   æ¨¡å‹: {result['model_name']}")
            print(f"   æ•°æ®é›†: {result['dataset']}")
            print(f"   æ ·æœ¬æ•°: {metrics['total_samples']}")
            print(f"   æ€»ä½“å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['correct_predictions']}/{metrics['total_samples']})")
            
            # Legitimateç±»åˆ«æŒ‡æ ‡
            leg_metrics = metrics['legitimate_metrics']
            print(f"\n   ğŸ“Š Legitimateç±»åˆ«æŒ‡æ ‡:")
            print(f"      Precision: {leg_metrics['precision']:.4f}")
            print(f"      Recall:    {leg_metrics['recall']:.4f}")
            print(f"      F1-Score:  {leg_metrics['f1_score']:.4f}")
            print(f"      TP: {leg_metrics['tp']}, FP: {leg_metrics['fp']}, FN: {leg_metrics['fn']}")
            
            # Fakeç±»åˆ«æŒ‡æ ‡
            fake_metrics = metrics['fake_metrics']
            print(f"\n   ğŸ“Š Fakeç±»åˆ«æŒ‡æ ‡:")
            print(f"      Precision: {fake_metrics['precision']:.4f}")
            print(f"      Recall:    {fake_metrics['recall']:.4f}")
            print(f"      F1-Score:  {fake_metrics['f1_score']:.4f}")
            print(f"      TP: {fake_metrics['tp']}, FP: {fake_metrics['fp']}, FN: {fake_metrics['fn']}")
            
            total_accuracy += metrics['accuracy']
        
        if len(task1_results) > 1:
            avg_accuracy = total_accuracy / len(task1_results)
            print(f"\nğŸ¯ Task1 å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
    
    # æ‰“å°Task3ç»“æœ
    if task3_results:
        print(f"\nğŸ“ˆ Task3 (äºŒåˆ†ç±»ä»»åŠ¡) - {len(task3_results)} ä¸ªæ–‡ä»¶:")
        total_accuracy = 0
        
        for result in task3_results:
            metrics = result['metrics']
            print(f"\nğŸ“ {result['file_path']}")
            print(f"   æ¨¡å‹: {result['model_name']}")
            print(f"   æ•°æ®é›†: {result['dataset']}")
            print(f"   æ ·æœ¬æ•°: {metrics['total_samples']}")
            print(f"   æ€»ä½“å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['correct_predictions']}/{metrics['total_samples']})")
            
            # Legitimateç±»åˆ«æŒ‡æ ‡
            leg_metrics = metrics['legitimate_metrics']
            print(f"\n   ğŸ“Š Legitimateç±»åˆ«æŒ‡æ ‡:")
            print(f"      Precision: {leg_metrics['precision']:.4f}")
            print(f"      Recall:    {leg_metrics['recall']:.4f}")
            print(f"      F1-Score:  {leg_metrics['f1_score']:.4f}")
            print(f"      TP: {leg_metrics['tp']}, FP: {leg_metrics['fp']}, FN: {leg_metrics['fn']}")
            
            # Fakeç±»åˆ«æŒ‡æ ‡
            fake_metrics = metrics['fake_metrics']
            print(f"\n   ğŸ“Š Fakeç±»åˆ«æŒ‡æ ‡:")
            print(f"      Precision: {fake_metrics['precision']:.4f}")
            print(f"      Recall:    {fake_metrics['recall']:.4f}")
            print(f"      F1-Score:  {fake_metrics['f1_score']:.4f}")
            print(f"      TP: {fake_metrics['tp']}, FP: {fake_metrics['fp']}, FN: {fake_metrics['fn']}")
            
            total_accuracy += metrics['accuracy']
        
        if len(task3_results) > 1:
            avg_accuracy = total_accuracy / len(task3_results)
            print(f"\nğŸ¯ Task3 å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
    
    # æ‰“å°Task2ç»“æœ
    if task2_results:
        print(f"\nğŸ“ˆ Task2 (äºŒåˆ†ç±»ä»»åŠ¡) - {len(task2_results)} ä¸ªæ–‡ä»¶:")
        total_accuracy = 0
        
        for result in task2_results:
            metrics = result['metrics']
            print(f"\nğŸ“ {result['file_path']}")
            print(f"   æ¨¡å‹: {result['model_name']}")
            print(f"   æ•°æ®é›†: {result['dataset']}")
            print(f"   å­ç±»: {result['task2_subclass']}")
            print(f"   æ ·æœ¬æ•°: {metrics['total_samples']}")
            print(f"   æ€»ä½“å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['correct_predictions']}/{metrics['total_samples']})")
            
            # æ ¹æ®å­ç±»ç±»å‹è°ƒæ•´æ ‡ç­¾æ˜¾ç¤º
            if "fake" in result['task2_subclass']:
                # å¯¹äºfakeå­ç±»ï¼šæ­£ç±» vs Other
                target_class = result['task2_subclass'].replace('_fake', '').replace('_', '-').title()
                print(f"\n   ğŸ“Š {target_class}ç±»åˆ«æŒ‡æ ‡ (æ­£ç±»):")
            else:
                # å¯¹äºlegitimateå­ç±»ï¼šStyle-based vs Integration-based
                print(f"\n   ğŸ“Š Style-Basedç±»åˆ«æŒ‡æ ‡:")
            
            leg_metrics = metrics['legitimate_metrics']
            print(f"      Precision: {leg_metrics['precision']:.4f}")
            print(f"      Recall:    {leg_metrics['recall']:.4f}")
            print(f"      F1-Score:  {leg_metrics['f1_score']:.4f}")
            print(f"      TP: {leg_metrics['tp']}, FP: {leg_metrics['fp']}, FN: {leg_metrics['fn']}")
            
            # è´Ÿç±»æŒ‡æ ‡
            if "fake" in result['task2_subclass']:
                print(f"\n   ğŸ“Š Otherç±»åˆ«æŒ‡æ ‡ (è´Ÿç±»):")
            else:
                print(f"\n   ğŸ“Š Integration-Basedç±»åˆ«æŒ‡æ ‡:")
            
            fake_metrics = metrics['fake_metrics']
            print(f"      Precision: {fake_metrics['precision']:.4f}")
            print(f"      Recall:    {fake_metrics['recall']:.4f}")
            print(f"      F1-Score:  {fake_metrics['f1_score']:.4f}")
            print(f"      TP: {fake_metrics['tp']}, FP: {fake_metrics['fp']}, FN: {fake_metrics['fn']}")
            
            total_accuracy += metrics['accuracy']
        
        if len(task2_results) > 1:
            avg_accuracy = total_accuracy / len(task2_results)
            print(f"\nğŸ¯ Task2 å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")

def main():
    parser = argparse.ArgumentParser(description='åˆ†ææ¨¡å‹é¢„æµ‹ç»“æœ')
    parser.add_argument('--dir', default='megafakeTasks', help='ç»“æœæ–‡ä»¶ç›®å½•')
    parser.add_argument('--file', help='æŒ‡å®šå•ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ')
    parser.add_argument('--output', default='megafakeTasks/results/prediction_analysis_results_base_models.csv', help='CSVè¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--no-csv', action='store_true', help='ä¸å¯¼å‡ºCSVæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹åˆ†ææ¨¡å‹é¢„æµ‹ç»“æœ")
    
    analysis_results = []
    
    if args.file:
        # åˆ†æå•ä¸ªæ–‡ä»¶
        if os.path.exists(args.file):
            result = analyze_predictions(args.file)
            if result:
                analysis_results.append(result)
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            return
    else:
        # æŸ¥æ‰¾å¹¶åˆ†ææ‰€æœ‰æ–‡ä»¶
        result_files = find_result_files(args.dir)
        
        if not result_files:
            print(f"âŒ åœ¨ç›®å½• {args.dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .jsonl æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")
        
        for file_path in result_files:
            result = analyze_predictions(file_path)
            if result:
                analysis_results.append(result)
    
    # æ‰“å°åˆ†æç»“æœ
    print_prediction_analysis(analysis_results)
    
    # å¯¼å‡ºCSVæ–‡ä»¶
    if not args.no_csv:
        export_to_csv(analysis_results, args.output)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main() 