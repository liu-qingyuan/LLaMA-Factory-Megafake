#!/usr/bin/env python3

import os
import subprocess
import yaml
import json
from pathlib import Path
import datetime
import tempfile
import argparse

REPO_ROOT = Path(__file__).resolve().parent.parent
HF_CACHE_DIR = REPO_ROOT / ".cache" / "huggingface"
HF_CACHE_DIR.mkdir(parents=True, exist_ok=True)
os.environ.setdefault("HF_HOME", str(HF_CACHE_DIR))
os.environ.setdefault("HF_DATASETS_CACHE", str(HF_CACHE_DIR / "datasets"))
os.environ.setdefault("HUGGINGFACE_HUB_CACHE", str(HF_CACHE_DIR / "hub"))

SA_ROOT = REPO_ROOT / "sensitivity_analysis"
SA_LOG_ROOT = SA_ROOT / "logs" / "train"
SA_OUTPUT_ROOT = SA_ROOT / "outputs"
LEGACY_OUTPUT_ROOT = REPO_ROOT / "megafakeTasks"
SA_LOG_ROOT.mkdir(parents=True, exist_ok=True)
SA_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
DATASET_INFO_PATH = REPO_ROOT / "data" / "dataset_info.json"
if DATASET_INFO_PATH.exists():
    with open(DATASET_INFO_PATH, "r", encoding="utf-8") as f:
        DATASET_INFO = json.load(f)
else:
    DATASET_INFO = {}


# æ¨¡å‹é…ç½®ï¼šæ¨¡å‹è·¯å¾„ -> (æ¨¡æ¿åç§°, trust_remote_code)
MODEL_CONFIGS = {
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": ("llama3", True),
    "/root/autodl-tmp/models/Qwen1.5-7B": ("qwen", True),
    "/root/autodl-tmp/models/chatglm3-6b": ("chatglm3", True),
    "/root/autodl-tmp/models/Mistral-7B-v0.1": ("mistral", False),
    "/root/autodl-tmp/models/Baichuan2-7B-Chat": ("baichuan2", True),
}
MODEL_NAME_MAP = {Path(path).name: path for path in MODEL_CONFIGS.keys()}

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    # æ—§ç‰ˆå¤§è§„æ¨¡å®éªŒä»å¯ç”¨ï¼štask1_small_*, task3_small_*, ...
    # "task1_full_glm": "task1_full_glm",
    # "task1_full_llama": "task1_full_llama",
    # "task1_small_glm": "task1_small_glm",
    # "task1_small_llama": "task1_small_llama",
    # "task3_full_gossip": "task3_full_gossip",
    # "task3_full_polifact": "task3_full_polifact",
    # "task3_small_gossip": "task3_small_gossip",
    # "task3_small_polifact": "task3_small_polifact",
    # Mini Test100 æ•°æ®é›†ï¼ˆ100æ­£/100è´Ÿï¼‰ç”¨äºå¿«é€ŸéªŒè¯æ•´æ¡æµæ°´çº¿
    "task1_test200_balanced_glm": "task1_test200_balanced_glm",
    # å¤§è§„æ¨¡å®éªŒæ•°æ®é›† (1k - 20k)
    "task1_scale_1000_glm": "task1_scale_1000_glm",
    "task1_scale_2000_glm": "task1_scale_2000_glm",
    "task1_scale_5000_glm": "task1_scale_5000_glm",
    "task1_scale_10000_glm": "task1_scale_10000_glm",
    "task1_scale_20000_glm": "task1_scale_20000_glm"
}

def get_model_name(model_path):
    """ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°"""
    return Path(model_path).name

def get_output_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆè¾“å‡ºè·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡å’Œç±»å‹
    if "task1" in dataset_name:
        task = "task1"
    elif "task2" in dataset_name:
        task = "task2"  
    elif "task3" in dataset_name:
        task = "task3"
    
    # æ„å»ºè¾“å‡ºè·¯å¾„
    # output_path = f"megafakeTasks/{task}/{dataset_name}/{model_name}/lora/sft"
    output_path = SA_OUTPUT_ROOT / task / dataset_name / model_name / "lora" / "sft"
    return output_path

def get_log_path(model_path, dataset_name):
    """ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    model_name = get_model_name(model_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"train_{model_name}_LoRA_{dataset_name}_{timestamp}.log"
    # return f"logs/{log_filename}"
    return SA_LOG_ROOT / log_filename

def create_config_file(model_path, template, trust_remote_code, dataset_name, output_path):
    """åˆ›å»ºä¸´æ—¶çš„è®­ç»ƒé…ç½®æ–‡ä»¶"""
    model_name = get_model_name(model_path)
    
    # åŸºç¡€é…ç½®æ¨¡æ¿
    config = {
        # model
        "model_name_or_path": model_path,
        "trust_remote_code": trust_remote_code,
        
        # method
        "stage": "sft",
        "do_train": True,
        "finetuning_type": "lora",
        "lora_rank": 16,
        "lora_target": "all",
        
        # dataset
        "dataset": dataset_name,
        "template": template,
        "cutoff_len": 2048,
        "max_samples": 8000,
        "overwrite_cache": True,
        "preprocessing_num_workers": 16,
        "dataloader_num_workers": 4,
        
        # output
        "output_dir": str(output_path),
        "logging_steps": 10,
        "save_steps": 1000,
        "plot_loss": True,
        "overwrite_output_dir": True,
        "save_only_model": False,
        "report_to": "none",
        
        # train
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 8,
        "learning_rate": 1.0e-4,
        "num_train_epochs": 1.0,
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.1,
        "bf16": False,
        "fp16": True,
        "ddp_timeout": 180000000,
        "resume_from_checkpoint": None
    }
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å†³å®šæ˜¯å¦å¯ç”¨Flash Attention 2.0
    # æ”¯æŒFA2çš„æ¨¡å‹ï¼šLLaMAç³»åˆ—ã€Qwenç³»åˆ—
    if any(name in model_name for name in ["Llama", "llama", "Qwen", "qwen"]):
        config["flash_attn"] = "fa2"
        print(f"âœ… ä¸ºæ¨¡å‹ {model_name} å¯ç”¨ Flash Attention 2.0")
    else:
        # Baichuanã€ChatGLMç­‰å¯èƒ½ä¸æ”¯æŒFA2
        print(f"âš ï¸  æ¨¡å‹ {model_name} ä¸å¯ç”¨ Flash Attention 2.0")
    
    # ä¸ºæŸäº›æ¨¡å‹è°ƒæ•´ç‰¹æ®Šé…ç½®
    if "Baichuan" in model_name:
        # Baichuanæ¨¡å‹çš„ç‰¹æ®Šé…ç½®
        pass
    elif "chatglm" in model_name.lower():
        # ChatGLMæ¨¡å‹çš„ç‰¹æ®Šé…ç½®
        pass
    elif "Mistral" in model_name:
        # Mistralæ¨¡å‹çš„ç‰¹æ®Šé…ç½®
        pass
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        return f.name

def run_training(model_path, template, trust_remote_code, dataset_name, output_path):
    """è¿è¡Œå•ä¸ªè®­ç»ƒä»»åŠ¡"""
    model_name = get_model_name(model_path)
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    config_file = create_config_file(model_path, template, trust_remote_code, dataset_name, output_path)
    
    try:
        # æ„å»ºåŒ…å«ç¯å¢ƒé…ç½®çš„å‘½ä»¤
        cmd = [
            "bash", "-c", 
            "export HF_ENDPOINT=https://hf-mirror.com && "
            "source /etc/network_turbo 2>/dev/null || true && "
            f"llamafactory-cli train {config_file}"
        ]
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•å’Œè¾“å‡ºç›®å½•
        log_path = get_log_path(model_path, dataset_name)
        full_output_path = output_path
        Path(log_path).parent.mkdir(parents=True, exist_ok=True)
        full_output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"é…ç½®æ–‡ä»¶: {config_file}")
        print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as log_file:
            # è®°å½•å‘½ä»¤å’Œæ—¶é—´æˆ³
            log_file.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
            log_file.write(f"é…ç½®æ–‡ä»¶: {config_file}\n")
            log_file.write("=" * 80 + "\n")
            log_file.flush()
            
            # ä½¿ç”¨ Popen æ¥å®æ—¶è¾“å‡ºæ—¥å¿—
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            # å®æ—¶è¯»å–è¾“å‡ºå¹¶åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
            for line in process.stdout:
                print(line, end='')  # è¾“å‡ºåˆ°æ§åˆ¶å°
                log_file.write(line)  # å†™å…¥æ–‡ä»¶
                log_file.flush()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            log_file.write("=" * 80 + "\n")
            log_file.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"è¿”å›ç : {return_code}\n")
            
            if return_code == 0:
                print(f"âœ… è®­ç»ƒæˆåŠŸ: {model_name} + {dataset_name}")
                print(f"   ä¿å­˜è‡³: {full_output_path.resolve()}")
                print(f"   æ—¥å¿—è‡³: {log_path.resolve()}")
                next_cmd = (
                    "python scripts/multi_model_lora_inference.py "
                    f"--models {model_name} --datasets {dataset_name}"
                )
                print(f"ğŸ‘‰ ä¸‹ä¸€æ­¥: {next_cmd}")
                return True, str(log_path.resolve())
            else:
                print(f"âŒ è®­ç»ƒå¤±è´¥: {model_name} + {dataset_name}")
                print(f"   è¿”å›ç : {return_code}")
                print(f"   æ—¥å¿—æ–‡ä»¶: {log_path.resolve()}")
                return False, str(log_path.resolve())
                
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {model_name} + {dataset_name}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {e}")
        return False, None
    finally:
        # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
        try:
            os.unlink(config_file)
        except:
            pass

def check_model_exists(model_path):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(model_path):
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    config_file = os.path.join(model_path, "config.json")
    if not os.path.exists(config_file):
        print(f"âš ï¸  æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    return True

def check_training_completed(output_path):
    """æ£€æŸ¥è®­ç»ƒæ˜¯å¦å·²ç»å®Œæˆ"""
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è®­ç»ƒå®Œæˆçš„æ ‡å¿—æ–‡ä»¶
    adapter_config = Path(output_path) / "adapter_config.json"
    adapter_model = Path(output_path) / "adapter_model.safetensors"
    if adapter_config.exists() and adapter_model.exists():
        return True
    try:
        relative = Path(output_path).relative_to(SA_OUTPUT_ROOT)
    except ValueError:
        return False
    legacy_path = LEGACY_OUTPUT_ROOT / relative
    legacy_config = legacy_path / "adapter_config.json"
    legacy_model = legacy_path / "adapter_model.safetensors"
    return legacy_config.exists() and legacy_model.exists()

def resolve_dataset_file(dataset_key):
    """è§£ææ•°æ®é›†æ–‡ä»¶è·¯å¾„"""
    info = DATASET_INFO.get(dataset_key)
    if not info:
        return None
    file_name = info.get("file_name")
    if not file_name:
        return None
    return REPO_ROOT / "data" / file_name

def dry_run_check(model_path, dataset_key, dataset_name):
    """Dry-run æ£€æŸ¥"""
    status_ok = True
    model_exists = check_model_exists(model_path)
    dataset_file = resolve_dataset_file(dataset_key)
    dataset_exists = dataset_file.exists() if dataset_file else False
    output_path = get_output_path(model_path, dataset_key)
    output_parent = output_path.parent
    print(f"\n[Dry-Run] æ¨¡å‹: {model_path}")
    print(f"[Dry-Run] æ•°æ®é›†ID: {dataset_key} ({dataset_name})")
    if dataset_file:
        print(f"[Dry-Run] æ•°æ®æ–‡ä»¶: {dataset_file} {'âœ…' if dataset_exists else 'âŒ'}")
    else:
        print(f"[Dry-Run] æ•°æ®æ–‡ä»¶: æœªåœ¨ dataset_info.json ä¸­æ‰¾åˆ°æ¡ç›® âŒ")
    print(f"[Dry-Run] è¾“å‡ºç›®å½•: {output_path.resolve()}")
    try:
        output_parent.mkdir(parents=True, exist_ok=True)
    except OSError as exc:
        print(f"[Dry-Run] æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {exc}")
        status_ok = False
    print(f"[Dry-Run] æ¨¡å‹æ£€æŸ¥: {'âœ…' if model_exists else 'âŒ'}")
    if not dataset_exists:
        status_ok = False
    if not model_exists:
        status_ok = False
    return status_ok

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æ‰¹é‡è¿è¡Œ LoRA è®­ç»ƒï¼ˆæ”¯æŒ Dry-Runï¼‰")
    parser.add_argument("--models", nargs="+", help="æŒ‡å®šæ¨¡å‹åç§°ï¼ˆå¦‚ Qwen1.5-7Bï¼‰æˆ–ç»å¯¹è·¯å¾„")
    parser.add_argument("--datasets", nargs="+", choices=list(DATASET_CONFIGS.keys()),
                        help="æŒ‡å®šæ•°æ®é›†é”®ï¼ˆé»˜è®¤å…¨é‡ï¼‰")
    parser.add_argument("--limit", type=int, help="é™åˆ¶ä»»åŠ¡æ•°é‡")
    parser.add_argument("--dry-run", action="store_true", help="ä»…æ£€æŸ¥è·¯å¾„ä¸ä¾èµ–ï¼Œä¸å®é™…è®­ç»ƒ")
    parser.add_argument("--include-large-models", action="store_true", help="å…è®¸è¿è¡Œ 70B+ å¤§æ¨¡å‹")
    return parser.parse_args()

def select_models(model_filters, include_large=False):
    """æ ¹æ®å‚æ•°ç­›é€‰æ¨¡å‹"""
    if not model_filters:
        items = list(MODEL_CONFIGS.items())
    else:
        items = []
        for item in model_filters:
            if item in MODEL_CONFIGS:
                items.append((item, MODEL_CONFIGS[item]))
                continue
            if item in MODEL_NAME_MAP:
                path = MODEL_NAME_MAP[item]
                items.append((path, MODEL_CONFIGS[path]))
                continue
            resolved = Path(item).expanduser()
            if str(resolved) in MODEL_CONFIGS:
                items.append((str(resolved), MODEL_CONFIGS[str(resolved)]))
            else:
                print(f"âš ï¸  æœªè¯†åˆ«çš„æ¨¡å‹: {item}")
    
    # è¿‡æ»¤å¤§æ¨¡å‹
    final_selection = []
    for path, config in items:
        if "72B" in str(path) or "70B" in str(path):
            if not include_large:
                print(f"âš ï¸  è·³è¿‡å¤§æ¨¡å‹ (éœ€ --include-large-models): {path}")
                continue
            else:
                print(f"âš ï¸  åŒ…å«å¤§æ¨¡å‹ (OOMé£é™©): {path}")
        final_selection.append((path, config))
        
    return final_selection

def select_datasets(dataset_filters):
    """æ ¹æ®å‚æ•°ç­›é€‰æ•°æ®é›†"""
    if not dataset_filters:
        return list(DATASET_CONFIGS.items())
    selected = []
    for key in dataset_filters:
        name = DATASET_CONFIGS.get(key)
        if name:
            selected.append((key, name))
        else:
            print(f"âš ï¸  æœªè¯†åˆ«çš„æ•°æ®é›†: {key}")
    return selected

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    selected_models = select_models(args.models, args.include_large_models)
    selected_datasets = select_datasets(args.datasets)
    total_tasks = len(selected_models) * len(selected_datasets)
    if args.limit:
        total_tasks = min(total_tasks, args.limit)
    print("ğŸš€ å¼€å§‹æ‰¹é‡Loraæ¨¡å‹è®­ç»ƒä»»åŠ¡")
    print(f"ğŸ“Š é€‰ä¸­ {len(selected_models)} ä¸ªæ¨¡å‹ï¼Œ{len(selected_datasets)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ¯ è®¡åˆ’ä»»åŠ¡æ•°: {total_tasks}")

    if args.dry_run:
        issues = False
        processed = 0
        for model_path, (template, trust_remote_code) in selected_models:
            for dataset_key, dataset_name in selected_datasets:
                if args.limit and processed >= args.limit:
                    break
                processed += 1
                ok = dry_run_check(model_path, dataset_key, dataset_name)
                if not ok:
                    issues = True
            if args.limit and processed >= args.limit:
                break
        if issues:
            print("\nâš ï¸  Dry-Run æ£€æµ‹åˆ°é—®é¢˜ï¼Œè¯·å…ˆä¿®å¤å†è¿è¡Œæ­£å¼è®­ç»ƒ")
            return
        print("\nâœ… Dry-Run æ£€æŸ¥é€šè¿‡ï¼Œå¯å®‰å…¨å¯åŠ¨è®­ç»ƒ")
        return

    completed_tasks = 0
    skipped_tasks = 0
    failed_tasks = 0
    processed_tasks = 0
    artifact_records = []
    for model_path, (template, trust_remote_code) in selected_models:
        model_name = get_model_name(model_path)
        print(f"\nğŸ”„ å¤„ç†æ¨¡å‹: {model_name} (æ¨¡æ¿: {template})")
        if not check_model_exists(model_path):
            print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {model_path}")
            failed_tasks += len(selected_datasets)
            continue
        for dataset_key, dataset_name in selected_datasets:
            if args.limit and processed_tasks >= args.limit:
                break
            processed_tasks += 1
            output_path = get_output_path(model_path, dataset_key)
            full_output_path = str(output_path)
            if check_training_completed(full_output_path):
                print(f"â­ï¸  è®­ç»ƒå·²å®Œæˆï¼Œè·³è¿‡: {model_name} + {dataset_key}")
                skipped_tasks += 1
                continue
            print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ: {model_name} + {dataset_key}")
            success, log_path = run_training(
                model_path,
                template,
                trust_remote_code,
                dataset_name,
                output_path
            )
            if success:
                completed_tasks += 1
                artifact_records.append({
                    "model": model_name,
                    "dataset": dataset_key,
                    "output": str(output_path),
                    "log": log_path
                })
            else:
                failed_tasks += 1
            print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks + failed_tasks + skipped_tasks}/{total_tasks} "
                  f"(æˆåŠŸ: {completed_tasks}, è·³è¿‡: {skipped_tasks}, å¤±è´¥: {failed_tasks})")
        if args.limit and processed_tasks >= args.limit:
            break

    print(f"\nğŸ‰ æ‰¹é‡è®­ç»ƒä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: {processed_tasks} ä¸ªä»»åŠ¡")
    print(f"âœ… æˆåŠŸ: {completed_tasks}")
    print(f"â­ï¸  è·³è¿‡: {skipped_tasks}")
    print(f"âŒ å¤±è´¥: {failed_tasks}")
    if failed_tasks > 0:
        print(f"âš ï¸  æœ‰ {failed_tasks} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
    if artifact_records:
        print("\nğŸ“¦ æœ¬æ¬¡æˆåŠŸäº§ç‰©:")
        for record in artifact_records:
            print(f"  - {record['model']} @ {record['dataset']}")
            print(f"    LoRAç›®å½•: {record['output']}")
            if record["log"]:
                print(f"    æ—¥å¿—: {record['log']}")
        model_args = " ".join(sorted({item["model"] for item in artifact_records}))
        dataset_args = " ".join(sorted({item["dataset"] for item in artifact_records}))
        next_cmd = (
            "python scripts/multi_model_lora_inference.py "
            f"--models {model_args} --datasets {dataset_args}"
        )
        print(f"\nğŸ”œ æ¨èä¸‹ä¸€æ­¥: {next_cmd}")
        print(f"ğŸ“ LoRA è¾“å‡ºæ ¹ç›®å½•: {SA_OUTPUT_ROOT}")

if __name__ == "__main__":
    main() 
