#!/usr/bin/env python3

import os
import subprocess
import yaml
import json
from pathlib import Path
import datetime
import tempfile

REPO_ROOT = Path(__file__).resolve().parent.parent
HF_CACHE_DIR = REPO_ROOT / ".cache" / "huggingface"
HF_CACHE_DIR.mkdir(parents=True, exist_ok=True)
os.environ.setdefault("HF_HOME", str(HF_CACHE_DIR))
os.environ.setdefault("HF_DATASETS_CACHE", str(HF_CACHE_DIR / "datasets"))
os.environ.setdefault("HUGGINGFACE_HUB_CACHE", str(HF_CACHE_DIR / "hub"))


# æ¨¡å‹é…ç½®ï¼šæ¨¡å‹è·¯å¾„ -> (æ¨¡æ¿åç§°, trust_remote_code)
MODEL_CONFIGS = {
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": ("llama3", True),
    "/root/autodl-tmp/models/Qwen1.5-7B": ("qwen", True),
    "/root/autodl-tmp/models/Qwen1.5-72B": ("qwen", True),
    "/root/autodl-tmp/models/chatglm3-6b": ("chatglm3", True),
    "/root/autodl-tmp/models/Mistral-7B-v0.1": ("mistral", False),
    "/root/autodl-tmp/models/Baichuan2-7B-Chat": ("baichuan2", True),
}

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    # æ—§ç‰ˆå¤§è§„æ¨¡å®éªŒä»å¯ç”¨ï¼štask1_small_*, task3_small_*, ...
    # "task1_full_glm": "task1_full_glm",
    # "task1_full_llama": "task1_full_llama",
    # "task1_small_glm": "task1_small_glm",
    # "task1_small_llama": "task1_small_llama",
    # "task3_full_gossip": "task3_full_gossip",
    # "task3_full_polifact": "task3_full_polifact",
    # "task3_small_gossip": "task3_small_gossip",
    # "task3_small_polifact": "task3_small_polifact",
    # Mini Test100 æ•°æ®é›†ï¼ˆ100æ­£/100è´Ÿï¼‰ç”¨äºå¿«é€ŸéªŒè¯æ•´æ¡æµæ°´çº¿
    "task1_test200_balanced_glm": "task1_test200_balanced_glm"
}

def get_model_name(model_path):
    """ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°"""
    return Path(model_path).name

def get_output_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆè¾“å‡ºè·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡å’Œç±»å‹
    if "task1" in dataset_name:
        task = "task1"
    elif "task2" in dataset_name:
        task = "task2"  
    elif "task3" in dataset_name:
        task = "task3"
    
    # æ„å»ºè¾“å‡ºè·¯å¾„
    output_path = f"megafakeTasks/{task}/{dataset_name}/{model_name}/lora/sft"
    return output_path

def get_log_path(model_path, dataset_name):
    """ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    model_name = get_model_name(model_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"train_{model_name}_LoRA_{dataset_name}_{timestamp}.log"
    return f"logs/{log_filename}"

def create_config_file(model_path, template, trust_remote_code, dataset_name, output_path):
    """åˆ›å»ºä¸´æ—¶çš„è®­ç»ƒé…ç½®æ–‡ä»¶"""
    model_name = get_model_name(model_path)
    
    # åŸºç¡€é…ç½®æ¨¡æ¿
    config = {
        # model
        "model_name_or_path": model_path,
        "trust_remote_code": trust_remote_code,
        
        # method
        "stage": "sft",
        "do_train": True,
        "finetuning_type": "lora",
        "lora_rank": 16,
        "lora_target": "all",
        
        # dataset
        "dataset": dataset_name,
        "template": template,
        "cutoff_len": 2048,
        "max_samples": 8000,
        "overwrite_cache": True,
        "preprocessing_num_workers": 16,
        "dataloader_num_workers": 4,
        
        # output
        "output_dir": str(REPO_ROOT / output_path),
        "logging_steps": 10,
        "save_steps": 1000,
        "plot_loss": True,
        "overwrite_output_dir": True,
        "save_only_model": False,
        "report_to": "none",
        
        # train
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 8,
        "learning_rate": 1.0e-4,
        "num_train_epochs": 1.0,
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.1,
        "bf16": False,
        "fp16": True,
        "ddp_timeout": 180000000,
        "resume_from_checkpoint": None
    }
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å†³å®šæ˜¯å¦å¯ç”¨Flash Attention 2.0
    # æ”¯æŒFA2çš„æ¨¡å‹ï¼šLLaMAç³»åˆ—ã€Qwenç³»åˆ—
    if any(name in model_name for name in ["Llama", "llama", "Qwen", "qwen"]):
        config["flash_attn"] = "fa2"
        print(f"âœ… ä¸ºæ¨¡å‹ {model_name} å¯ç”¨ Flash Attention 2.0")
    else:
        # Baichuanã€ChatGLMç­‰å¯èƒ½ä¸æ”¯æŒFA2
        print(f"âš ï¸  æ¨¡å‹ {model_name} ä¸å¯ç”¨ Flash Attention 2.0")
    
    # ä¸ºæŸäº›æ¨¡å‹è°ƒæ•´ç‰¹æ®Šé…ç½®
    if "Baichuan" in model_name:
        # Baichuanæ¨¡å‹çš„ç‰¹æ®Šé…ç½®
        pass
    elif "chatglm" in model_name.lower():
        # ChatGLMæ¨¡å‹çš„ç‰¹æ®Šé…ç½®
        pass
    elif "Mistral" in model_name:
        # Mistralæ¨¡å‹çš„ç‰¹æ®Šé…ç½®
        pass
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        return f.name

def run_training(model_path, template, trust_remote_code, dataset_name, output_path):
    """è¿è¡Œå•ä¸ªè®­ç»ƒä»»åŠ¡"""
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    config_file = create_config_file(model_path, template, trust_remote_code, dataset_name, output_path)
    
    try:
        # æ„å»ºåŒ…å«ç¯å¢ƒé…ç½®çš„å‘½ä»¤
        cmd = [
            "bash", "-c", 
            "export HF_ENDPOINT=https://hf-mirror.com && "
            "source /etc/network_turbo 2>/dev/null || true && "
            f"llamafactory-cli train {config_file}"
        ]
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•å’Œè¾“å‡ºç›®å½•
        log_path = get_log_path(model_path, dataset_name)
        full_output_path = str(REPO_ROOT / output_path)
        os.makedirs(os.path.dirname(log_path), exist_ok=True)
        os.makedirs(full_output_path, exist_ok=True)
        
        print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"é…ç½®æ–‡ä»¶: {config_file}")
        print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as log_file:
            # è®°å½•å‘½ä»¤å’Œæ—¶é—´æˆ³
            log_file.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
            log_file.write(f"é…ç½®æ–‡ä»¶: {config_file}\n")
            log_file.write("=" * 80 + "\n")
            log_file.flush()
            
            # ä½¿ç”¨ Popen æ¥å®æ—¶è¾“å‡ºæ—¥å¿—
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            # å®æ—¶è¯»å–è¾“å‡ºå¹¶åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
            for line in process.stdout:
                print(line, end='')  # è¾“å‡ºåˆ°æ§åˆ¶å°
                log_file.write(line)  # å†™å…¥æ–‡ä»¶
                log_file.flush()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            log_file.write("=" * 80 + "\n")
            log_file.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"è¿”å›ç : {return_code}\n")
            
            if return_code == 0:
                print(f"âœ… è®­ç»ƒæˆåŠŸ: {get_model_name(model_path)} + {dataset_name}")
                print(f"   ä¿å­˜è‡³: {output_path}")
                print(f"   æ—¥å¿—è‡³: {log_path}")
                return True
            else:
                print(f"âŒ è®­ç»ƒå¤±è´¥: {get_model_name(model_path)} + {dataset_name}")
                print(f"   è¿”å›ç : {return_code}")
                print(f"   æ—¥å¿—æ–‡ä»¶: {log_path}")
                return False
                
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {get_model_name(model_path)} + {dataset_name}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {e}")
        return False
    finally:
        # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
        try:
            os.unlink(config_file)
        except:
            pass

def check_model_exists(model_path):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(model_path):
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    config_file = os.path.join(model_path, "config.json")
    if not os.path.exists(config_file):
        print(f"âš ï¸  æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    return True

def check_training_completed(output_path):
    """æ£€æŸ¥è®­ç»ƒæ˜¯å¦å·²ç»å®Œæˆ"""
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è®­ç»ƒå®Œæˆçš„æ ‡å¿—æ–‡ä»¶
    adapter_config = os.path.join(output_path, "adapter_config.json")
    adapter_model = os.path.join(output_path, "adapter_model.safetensors")
    
    return os.path.exists(adapter_config) and os.path.exists(adapter_model)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡Loraæ¨¡å‹è®­ç»ƒä»»åŠ¡")
    print(f"ğŸ“Š æ€»è®¡ {len(MODEL_CONFIGS)} ä¸ªæ¨¡å‹ï¼Œ{len(DATASET_CONFIGS)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ¯ æ€»ä»»åŠ¡æ•°: {len(MODEL_CONFIGS) * len(DATASET_CONFIGS)}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_tasks = len(MODEL_CONFIGS) * len(DATASET_CONFIGS)
    completed_tasks = 0
    skipped_tasks = 0
    failed_tasks = 0
    
    # éå†æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ç»„åˆ
    for model_path, (template, trust_remote_code) in MODEL_CONFIGS.items():
        model_name = get_model_name(model_path)
        print(f"\nğŸ”„ å¤„ç†æ¨¡å‹: {model_name} (æ¨¡æ¿: {template})")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not check_model_exists(model_path):
            print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {model_path}")
            failed_tasks += len(DATASET_CONFIGS)
            continue
        
        for dataset_key, dataset_name in DATASET_CONFIGS.items():
            output_path = get_output_path(model_path, dataset_key)
            full_output_path = f"/root/autodl-tmp/LLaMA-Factory-Megafake2/{output_path}"
            
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè®­ç»ƒ
            if check_training_completed(full_output_path):
                print(f"â­ï¸  è®­ç»ƒå·²å®Œæˆï¼Œè·³è¿‡: {model_name} + {dataset_key}")
                skipped_tasks += 1
                continue
            
            # è¿è¡Œè®­ç»ƒ
            print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ: {model_name} + {dataset_key}")
            success = run_training(model_path, template, trust_remote_code, dataset_name, output_path)
            
            if success:
                completed_tasks += 1
            else:
                failed_tasks += 1
            
            print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks + failed_tasks + skipped_tasks}/{total_tasks} "
                  f"(æˆåŠŸ: {completed_tasks}, è·³è¿‡: {skipped_tasks}, å¤±è´¥: {failed_tasks})")
    
    print(f"\nğŸ‰ æ‰¹é‡è®­ç»ƒä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: {total_tasks} ä¸ªä»»åŠ¡")
    print(f"âœ… æˆåŠŸ: {completed_tasks}")
    print(f"â­ï¸  è·³è¿‡: {skipped_tasks}")
    print(f"âŒ å¤±è´¥: {failed_tasks}")
    
    if failed_tasks > 0:
        print(f"âš ï¸  æœ‰ {failed_tasks} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main() 
