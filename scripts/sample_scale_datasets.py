#!/usr/bin/env python3
import json
import random
import os
from pathlib import Path

def sample_and_register_datasets():
    """
    ä»å…¨é‡æ•°æ®ä¸­é‡‡æ ·ä¸åŒè§„æ¨¡çš„æ•°æ®é›† (1k, 2k, 5k, 10k, 20k)
    å¹¶è‡ªåŠ¨æ³¨å†Œåˆ° dataset_info.json
    """
    
    # é…ç½®
    SOURCE_KEY = "task1_full_glm"
    TARGET_SCALES = [1000, 2000, 5000, 10000, 20000]
    REPO_ROOT = Path(__file__).resolve().parent.parent
    DATA_INFO_PATH = REPO_ROOT / "data" / "dataset_info.json"
    
    # 1. è¯»å– dataset_info.json è·å–æºæ–‡ä»¶è·¯å¾„
    with open(DATA_INFO_PATH, 'r', encoding='utf-8') as f:
        dataset_info = json.load(f)
        
    if SOURCE_KEY not in dataset_info:
        print(f"âŒ æºæ•°æ®é›† {SOURCE_KEY} æœªåœ¨ dataset_info.json ä¸­æ‰¾åˆ°")
        return

    source_rel_path = dataset_info[SOURCE_KEY]["file_name"]
    source_full_path = REPO_ROOT / "data" / source_rel_path
    
    if not source_full_path.exists():
        print(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {source_full_path}")
        return

    print(f"ğŸ“– æ­£åœ¨è¯»å–æºæ–‡ä»¶: {source_full_path}")
    with open(source_full_path, 'r', encoding='utf-8') as f:
        full_data = json.load(f)
    
    total_count = len(full_data)
    print(f"ğŸ“Š å…¨é‡æ•°æ®æ€»æ•°: {total_count}")
    
    # 2. åˆ†ç¦»æ­£è´Ÿæ ·æœ¬ä»¥ç¡®ä¿å¹³è¡¡
    legitimate_samples = [x for x in full_data if x.get('output') == 'legitimate']
    fake_samples = [x for x in full_data if x.get('output') == 'fake']
    
    print(f"   - Legitimate: {len(legitimate_samples)}")
    print(f"   - Fake: {len(fake_samples)}")
    
    # 3. å¾ªç¯ç”Ÿæˆå„è§„æ¨¡æ•°æ®é›†
    output_base_dir = REPO_ROOT / "data" / "data_table" / "task1" / "scale_experiment"
    output_base_dir.mkdir(parents=True, exist_ok=True)
    
    random.seed(42) # å›ºå®šç§å­
    
    new_registrations = {}
    
    for scale in TARGET_SCALES:
        if scale > total_count:
            print(f"âš ï¸  è·³è¿‡è§„æ¨¡ {scale}: è¶…è¿‡å…¨é‡æ•°æ® ({total_count})")
            continue
            
        # å°è¯•å¹³è¡¡é‡‡æ ·
        half_scale = scale // 2
        if len(legitimate_samples) >= half_scale and len(fake_samples) >= half_scale:
            # å¯ä»¥å®Œå…¨å¹³è¡¡
            selected_leg = random.sample(legitimate_samples, half_scale)
            selected_fake = random.sample(fake_samples, half_scale)
            selected_data = selected_leg + selected_fake
        else:
            # æ— æ³•å®Œå…¨å¹³è¡¡ï¼Œé€€åŒ–ä¸ºéšæœºé‡‡æ ·
            print(f"âš ï¸  è§„æ¨¡ {scale} æ— æ³•å®Œå…¨å¹³è¡¡ (æ­£/è´Ÿæ ·æœ¬ä¸è¶³ {half_scale})ï¼Œé‡‡ç”¨å…¨å±€éšæœºé‡‡æ ·")
            selected_data = random.sample(full_data, scale)
            
        random.shuffle(selected_data) # æ‰“ä¹±é¡ºåº
        
        # ç”Ÿæˆæ–‡ä»¶åå’Œ Key
        filename = f"alpaca_megafake_glm_{scale}.json"
        dataset_key = f"task1_scale_{scale}_glm"
        output_path = output_base_dir / filename
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(selected_data, f, ensure_ascii=False, indent=2)
            
        # è®°å½•ç›¸å¯¹è·¯å¾„ç”¨äºæ³¨å†Œ
        rel_path = str(output_path.relative_to(REPO_ROOT / "data"))
        dataset_info[dataset_key] = {"file_name": rel_path}
        new_registrations[dataset_key] = rel_path
        
        # ç»Ÿè®¡å½“å‰æ•°æ®é›†
        curr_leg = sum(1 for x in selected_data if x.get('output') == 'legitimate')
        curr_fake = sum(1 for x in selected_data if x.get('output') == 'fake')
        print(f"âœ… ç”Ÿæˆ {dataset_key}: {len(selected_data)} æ¡ (Leg: {curr_leg}, Fake: {curr_fake}) -> {rel_path}")

    # 4. æ›´æ–° dataset_info.json
    print(f"ğŸ’¾ æ›´æ–° dataset_info.json ...")
    with open(DATA_INFO_PATH, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        
    print("ğŸ‰ æ‰€æœ‰å¤§è§„æ¨¡æ•°æ®é›†å·²ç”Ÿæˆå¹¶æ³¨å†Œã€‚")
    print("ğŸ‘‰ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ key è¿›è¡Œå®éªŒ:")
    for key in new_registrations:
        print(f"   - {key}")

if __name__ == "__main__":
    sample_and_register_datasets()
