#!/usr/bin/env python3

import os
import subprocess
import json
from pathlib import Path
import datetime
import argparse

REPO_ROOT = Path(__file__).resolve().parent.parent
SA_ROOT = REPO_ROOT / "sensitivity_analysis"
SA_LOG_ROOT = SA_ROOT / "logs" / "infer"
SA_OUTPUT_ROOT = SA_ROOT / "outputs"
LEGACY_OUTPUT_ROOT = REPO_ROOT / "megafakeTasks"
SA_LOG_ROOT.mkdir(parents=True, exist_ok=True)
SA_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)

DATASET_INFO_PATH = REPO_ROOT / "data" / "dataset_info.json"
if DATASET_INFO_PATH.exists():
    with open(DATASET_INFO_PATH, "r", encoding="utf-8") as f:
        DATASET_INFO = json.load(f)
else:
    DATASET_INFO = {}

# æ¨¡å‹é…ç½®ï¼šæ¨¡å‹è·¯å¾„ -> æ¨¡æ¿åç§°
MODEL_CONFIGS = {
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": "llama3",
    "/root/autodl-tmp/models/Qwen1.5-7B": "qwen",
    "/root/autodl-tmp/models/chatglm3-6b": "chatglm3",
    "/root/autodl-tmp/models/Mistral-7B-v0.1": "mistral",
    "/root/autodl-tmp/models/Baichuan2-7B-Chat": "baichuan2",
}
MODEL_NAME_MAP = {Path(path).name: path for path in MODEL_CONFIGS.keys()}

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    "task1_test200_balanced_glm": "data_table/task1/alpaca_test100_balanced/alpaca_megafake_glm_test200_balanced.json",
}


def get_model_name(model_path: str) -> str:
    """ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°"""
    return Path(model_path).name


def build_relative_output(dataset_name: str, model_name: str) -> Path:
    """æ„å»ºè¾“å‡ºæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼ˆä¸å«æ ¹ç›®å½•ï¼‰"""
    
    if "task1" in dataset_name:
        task = "task1"
        if "cot_sc" in dataset_name:
            reasoning_type = "CoT_SC"
        elif "fs_5" in dataset_name:
            reasoning_type = "FS_5"
        elif "zs_df" in dataset_name:
            reasoning_type = "ZS_DF"
        else:
            reasoning_type = None

        if "full" in dataset_name:
            size = "full"
        elif "test100" in dataset_name:
            size = "test100"
        elif "test200" in dataset_name:
            size = "test200_balanced"
        else:
            size = "small"

        if "glm" in dataset_name:
            if reasoning_type:
                if size in ("test100", "test200_balanced"):
                    data_type = f"test100_{reasoning_type.lower()}_megafake_glm_binary"
                else:
                    data_type = f"{reasoning_type.lower()}_megafake_glm_binary"
            else:
                data_type = "megafake_glm_binary" if size != "test200_balanced" else "test200_balanced_megafake_glm_binary"
        else:
            if reasoning_type:
                if size == "test100":
                    data_type = f"test100_{reasoning_type.lower()}_megafake_llama_binary"
                else:
                    data_type = f"{reasoning_type.lower()}_megafake_llama_binary"
            else:
                data_type = "megafake_llama_binary"

    elif "task2" in dataset_name:
        task = "task2"
        size = "full" if "full" in dataset_name else "small"
        parts = dataset_name.split("_")
        model_source = parts[2]
        news_type = parts[-1]
        subclass_parts = parts[3:-1]
        if "based" in subclass_parts:
            subclass_parts.remove("based")
        subclass = "_".join(subclass_parts)
        if model_source == "glm":
            data_type = f"glm_{subclass}_based_{news_type}"
        else:
            data_type = f"llama3_{subclass}_based_{news_type}"

    else:
        task = "task3"
        size = "full" if "full" in dataset_name else "small"
        data_type = "chatglm_gossip_binary" if "gossip" in dataset_name else "chatglm_polifact_binary"

    relative = Path(task) / size / f"result_{data_type}_{model_name}.jsonl"
    return relative


def get_save_path(model_path: str, dataset_name: str) -> Path:
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆä¿å­˜è·¯å¾„"""
    model_name = get_model_name(model_path)
    relative = build_relative_output(dataset_name, model_name)
    return SA_OUTPUT_ROOT / relative


def get_legacy_save_path(model_path: str, dataset_name: str) -> Path:
    model_name = get_model_name(model_path)
    relative = build_relative_output(dataset_name, model_name)
    return LEGACY_OUTPUT_ROOT / relative


def get_log_path(model_path: str, dataset_name: str) -> Path:
    model_name = get_model_name(model_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"inference_{model_name}_{dataset_name}_{timestamp}.log"
    return SA_LOG_ROOT / log_filename


def run_inference(model_path: str,
                  template: str,
                  dataset_name: str,
                  save_path: Path,
                  max_new_tokens: int | None = None) -> bool:
    if max_new_tokens is None:
        if "cot_sc" in dataset_name:
            max_new_tokens = 512
        elif "zs_df" in dataset_name:
            max_new_tokens = 256
        elif "fs_5" in dataset_name:
            max_new_tokens = 128
        elif "test200" in dataset_name:
            max_new_tokens = 64
        else:
            max_new_tokens = 30

    cmd = [
        "python", "scripts/vllm_infer.py",
        "--model_name_or_path", model_path,
        "--template", template,
        "--dataset", dataset_name,
        "--save_name", str(save_path),
        "--max_new_tokens", str(max_new_tokens),
        "--temperature", "0.1",
        "--top_p", "0.9",
        "--batch_size", "1024"
    ]

    model_name = get_model_name(model_path)
    if "Baichuan" in model_name or "chatglm" in model_name.lower():
        cmd.append("--trust_remote_code")

    log_path = get_log_path(model_path, dataset_name)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")

    try:
        env_cmd = [
            "bash", "-c",
            "export HF_ENDPOINT=https://hf-mirror.com && "
            "source /etc/network_turbo 2>/dev/null || true && "
            f"{' '.join(cmd)}"
        ]

        with open(log_path, "w", encoding="utf-8") as log_file:
            log_file.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
            log_file.write("=" * 80 + "\n")
            log_file.flush()

            process = subprocess.Popen(
                env_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )

            for line in process.stdout:
                print(line, end="")
                log_file.write(line)
                log_file.flush()

            return_code = process.wait()
            log_file.write("=" * 80 + "\n")
            log_file.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"è¿”å›ç : {return_code}\n")

        if return_code == 0:
            print(f"âœ… æˆåŠŸå®Œæˆ: {model_name} + {dataset_name}")
            print(f"   ä¿å­˜è‡³: {save_path.resolve()}")
            print(f"   æ—¥å¿—è‡³: {log_path.resolve()}")
            next_cmd = (
                f"python scripts/analyze_predictions.py --file {save_path.resolve()} "
                f"--output sensitivity_analysis/results/{save_path.stem}_metrics.csv"
            )
            print(f"ğŸ‘‰ ä¸‹ä¸€æ­¥: {next_cmd}")
            return True
        else:
            print(f"âŒ å¤±è´¥: {model_name} + {dataset_name}")
            print(f"   è¿”å›ç : {return_code}")
            print(f"   æ—¥å¿—æ–‡ä»¶: {log_path}")
            return False
    except Exception as exc:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {model_name} + {dataset_name}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {exc}")
        return False


def check_model_exists(model_path: str) -> bool:
    if not os.path.exists(model_path):
        return False
    config_file = os.path.join(model_path, "config.json")
    if not os.path.exists(config_file):
        print(f"âš ï¸  æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    return True


def resolve_dataset_file(dataset_key: str) -> Path | None:
    info = DATASET_INFO.get(dataset_key)
    if not info:
        return None
    file_name = info.get("file_name")
    if not file_name:
        return None
    return REPO_ROOT / "data" / file_name


def dry_run_check(model_path: str, dataset_key: str, dataset_rel: str) -> bool:
    dataset_file = resolve_dataset_file(dataset_key)
    dataset_ready = dataset_file.exists() if dataset_file else False
    save_path = get_save_path(model_path, dataset_key)
    legacy_path = get_legacy_save_path(model_path, dataset_key)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"\n[Dry-Run] æ¨¡å‹: {model_path}")
    if dataset_file:
        print(f"[Dry-Run] æ•°æ®æ–‡ä»¶: {dataset_file} {'âœ…' if dataset_ready else 'âŒ'}")
    else:
        print(f"[Dry-Run] æ•°æ®æ–‡ä»¶: æœªåœ¨ dataset_info.json ä¸­ç™»è®° âŒ")
    print(f"[Dry-Run] è¾“å‡ºæ–‡ä»¶: {save_path.resolve()}")
    if legacy_path.exists():
        print(f"[Dry-Run] å…¼å®¹ï¼šæ£€æµ‹åˆ°å†å²ç»“æœ {legacy_path.resolve()}")
    if not dataset_ready:
        return False
    return check_model_exists(model_path)


def parse_args():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è¿è¡ŒåŸºç¡€æ¨¡å‹æ¨ç†ï¼ˆæ—  LoRAï¼‰")
    parser.add_argument("--models", nargs="+", help="æŒ‡å®šæ¨¡å‹åç§°æˆ–ç»å¯¹è·¯å¾„")
    parser.add_argument("--datasets", nargs="+", choices=list(DATASET_CONFIGS.keys()),
                        help="æŒ‡å®šæ•°æ®é›†é”®")
    parser.add_argument("--limit", type=int, help="é™åˆ¶æ‰§è¡Œä»»åŠ¡æ•°é‡")
    parser.add_argument("--dry-run", action="store_true", help="ä»…æ£€æŸ¥ä¾èµ–ï¼Œä¸å®é™…æ¨ç†")
    parser.add_argument("--skip-existing", action="store_true", help="è‹¥å­˜åœ¨ç»“æœåˆ™è·³è¿‡")
    parser.add_argument("--max-new-tokens", type=int, help="è¦†ç›–é»˜è®¤ max_new_tokens")
    return parser.parse_args()


def select_models(model_filters):
    if not model_filters:
        return list(MODEL_CONFIGS.items())
    selected = []
    for item in model_filters:
        if item in MODEL_CONFIGS:
            selected.append((item, MODEL_CONFIGS[item]))
            continue
        if item in MODEL_NAME_MAP:
            path = MODEL_NAME_MAP[item]
            selected.append((path, MODEL_CONFIGS[path]))
            continue
        resolved = Path(item).expanduser()
        if str(resolved) in MODEL_CONFIGS:
            selected.append((str(resolved), MODEL_CONFIGS[str(resolved)]))
        else:
            print(f"âš ï¸  æœªè¯†åˆ«çš„æ¨¡å‹: {item}")
    return selected


def select_datasets(dataset_filters):
    if not dataset_filters:
        return list(DATASET_CONFIGS.items())
    selected = []
    for key in dataset_filters:
        rel = DATASET_CONFIGS.get(key)
        if rel:
            selected.append((key, rel))
        else:
            print(f"âš ï¸  æœªè¯†åˆ«çš„æ•°æ®é›†: {key}")
    return selected


def main():
    args = parse_args()
    selected_models = select_models(args.models)
    selected_datasets = select_datasets(args.datasets)
    total_tasks = len(selected_models) * len(selected_datasets)
    if args.limit:
        total_tasks = min(total_tasks, args.limit)

    print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹æ¨ç†ä»»åŠ¡")
    print(f"ğŸ“Š é€‰ä¸­ {len(selected_models)} ä¸ªæ¨¡å‹ï¼Œ{len(selected_datasets)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ¯ è®¡åˆ’ä»»åŠ¡æ•°: {total_tasks}")

    if args.dry_run:
        issues = False
        processed = 0
        for model_path, template in selected_models:
            for dataset_key, dataset_rel in selected_datasets:
                if args.limit and processed >= args.limit:
                    break
                processed += 1
                ok = dry_run_check(model_path, dataset_key, dataset_rel)
                if not ok:
                    issues = True
            if args.limit and processed >= args.limit:
                break
        if issues:
            print("\nâš ï¸  Dry-Run æ£€æµ‹åˆ°é—®é¢˜ï¼Œè¯·ä¿®å¤åå†è¿è¡Œ")
            return
        print("\nâœ… Dry-Run æ£€æŸ¥é€šè¿‡ï¼Œå¯å®‰å…¨å¯åŠ¨æ¨ç†")
        return

    completed_tasks = 0
    failed_tasks = 0
    processed_tasks = 0

    for model_path, template in selected_models:
        model_name = get_model_name(model_path)
        print(f"\nğŸ”„ å¤„ç†æ¨¡å‹: {model_name} (æ¨¡æ¿: {template})")
        if not check_model_exists(model_path):
            print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {model_path}")
            failed_tasks += len(selected_datasets)
            continue

        for dataset_key, dataset_rel in selected_datasets:
            if args.limit and processed_tasks >= args.limit:
                break
            processed_tasks += 1
            save_path = get_save_path(model_path, dataset_key)
            legacy_path = get_legacy_save_path(model_path, dataset_key)
            save_path.parent.mkdir(parents=True, exist_ok=True)

            if args.skip_existing and (save_path.exists() or legacy_path.exists()):
                existing_path = save_path if save_path.exists() else legacy_path
                print(f"â­ï¸  ç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {existing_path}")
                completed_tasks += 1
                continue

            print(f"ğŸ¯ å¼€å§‹æ¨ç†: {model_name} + {dataset_key}")
            success = run_inference(
                model_path,
                template,
                dataset_key,
                save_path,
                max_new_tokens=args.max_new_tokens
            )
            if success:
                completed_tasks += 1
            else:
                failed_tasks += 1

            print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks + failed_tasks}/{total_tasks} "
                  f"(æˆåŠŸ: {completed_tasks}, å¤±è´¥: {failed_tasks})")

        if args.limit and processed_tasks >= args.limit:
            break

    print("\nğŸ‰ å¤šæ¨¡å‹æ¨ç†ä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: {processed_tasks} ä¸ªä»»åŠ¡")
    print(f"âœ… æˆåŠŸ: {completed_tasks}")
    print(f"âŒ å¤±è´¥: {failed_tasks}")
    if failed_tasks > 0:
        print(f"âš ï¸  æœ‰ {failed_tasks} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")


if __name__ == "__main__":
    main()
