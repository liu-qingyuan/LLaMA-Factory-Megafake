#!/usr/bin/env python3

import os
import subprocess
import json
from pathlib import Path
import datetime

# æ¨¡å‹é…ç½®ï¼šæ¨¡å‹è·¯å¾„ -> æ¨¡æ¿åç§°
MODEL_CONFIGS = {
    "/root/autodl-tmp/models/Baichuan2-7B-Chat": "baichuan2",
    "/root/autodl-tmp/models/chatglm3-6b": "chatglm3", 
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": "llama3",
    "/root/autodl-tmp/models/Mistral-7B-v0.1": "mistral",
    "/root/autodl-tmp/models/Qwen1.5-7B": "qwen"
}

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    # "task1_full_glm": "data_table/task1/alpaca_full/alpaca_megafake_glm_binary.json",
    # "task1_full_llama": "data_table/task1/alpaca_full/alpaca_megafake_llama_binary.json", 
    # "task1_small_glm": "data_table/task1/small_8k/alpaca_megafake_glm_8k.json",
    # "task1_small_llama": "data_table/task1/small_8k/alpaca_megafake_llama_8k.json",
    
    # Task2 - GLM å‡æ–°é—»å­ç±»
    "task2_full_glm_style_based_fake": "data_table/task2/alpaca_full/glm/alpaca_glm_style_based_fake.json",
    "task2_full_glm_content_based_fake": "data_table/task2/alpaca_full/glm/alpaca_glm_content_based_fake.json",
    "task2_full_glm_integration_based_fake": "data_table/task2/alpaca_full/glm/alpaca_glm_integration_based_fake.json",
    "task2_full_glm_story_based_fake": "data_table/task2/alpaca_full/glm/alpaca_glm_story_based_fake.json",
    # Task2 - GLM çœŸæ–°é—»å­ç±»
    "task2_full_glm_style_based_legitimate": "data_table/task2/alpaca_full/glm/alpaca_glm_style_based_legitimate.json",
    "task2_full_glm_integration_based_legitimate": "data_table/task2/alpaca_full/glm/alpaca_glm_integration_based_legitimate.json",
    
    # Task2 - LLaMA å‡æ–°é—»å­ç±»
    "task2_full_llama_style_based_fake": "data_table/task2/alpaca_full/llama/alpaca_llama3_style_based_fake.json",
    "task2_full_llama_content_based_fake": "data_table/task2/alpaca_full/llama/alpaca_llama3_content_based_fake.json",
    "task2_full_llama_integration_based_fake": "data_table/task2/alpaca_full/llama/alpaca_llama3_integration_based_fake.json",
    "task2_full_llama_story_based_fake": "data_table/task2/alpaca_full/llama/alpaca_llama3_story_based_fake.json",
    # Task2 - LLaMA çœŸæ–°é—»å­ç±»
    "task2_full_llama_style_based_legitimate": "data_table/task2/alpaca_full/llama/alpaca_llama3_style_based_legitimate.json",
    "task2_full_llama_integration_based_legitimate": "data_table/task2/alpaca_full/llama/alpaca_llama3_integration_based_legitimate.json",
    
    # Task2 - 8K é‡‡æ ·æ•°æ®é›†ï¼ˆæ³¨é‡Šæ‰ä»¥å‡å°‘ä»»åŠ¡é‡ï¼Œéœ€è¦æ—¶å¯å–æ¶ˆæ³¨é‡Šï¼‰
    # "task2_small_glm_style_based_fake": "data_table/task2/small_8k/glm/alpaca_glm_style_based_fake_8k.json",
    # "task2_small_glm_content_based_fake": "data_table/task2/small_8k/glm/alpaca_glm_content_based_fake_8k.json",
    # "task2_small_glm_integration_based_fake": "data_table/task2/small_8k/glm/alpaca_glm_integration_based_fake_8k.json",
    # "task2_small_glm_story_based_fake": "data_table/task2/small_8k/glm/alpaca_glm_story_based_fake_8k.json",
    # "task2_small_glm_style_based_legitimate": "data_table/task2/small_8k/glm/alpaca_glm_style_based_legitimate_8k.json",
    # "task2_small_glm_integration_based_legitimate": "data_table/task2/small_8k/glm/alpaca_glm_integration_based_legitimate_8k.json",
    # "task2_small_llama_style_based_fake": "data_table/task2/small_8k/llama/alpaca_llama3_style_based_fake_8k.json",
    # "task2_small_llama_content_based_fake": "data_table/task2/small_8k/llama/alpaca_llama3_content_based_fake_8k.json",
    # "task2_small_llama_integration_based_fake": "data_table/task2/small_8k/llama/alpaca_llama3_integration_based_fake_8k.json",
    # "task2_small_llama_story_based_fake": "data_table/task2/small_8k/llama/alpaca_llama3_story_based_fake_8k.json",
    # "task2_small_llama_style_based_legitimate": "data_table/task2/small_8k/llama/alpaca_llama3_style_based_legitimate_8k.json",
    # "task2_small_llama_integration_based_legitimate": "data_table/task2/small_8k/llama/alpaca_llama3_integration_based_legitimate_8k.json",
    
    # "task3_full_gossip": "data_table/task3/alpaca_full/alpaca_chatglm_gossip_binary.json",
    # "task3_full_polifact": "data_table/task3/alpaca_full/alpaca_chatglm_polifact_binary.json",
    # "task3_small_gossip": "data_table/task3/small_8k/alpaca_chatglm_gossip_8k.json",
    # "task3_small_polifact": "data_table/task3/small_8k/alpaca_chatglm_polifact_8k.json"
}

def get_model_name(model_path):
    """ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°"""
    return Path(model_path).name

def get_save_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆä¿å­˜è·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡å’Œç±»å‹
    if "task1" in dataset_name:
        task = "task1"
        if "full" in dataset_name:
            size = "full"
        else:
            size = "small"
        
        if "glm" in dataset_name:
            data_type = "megafake_glm_binary"
        else:
            data_type = "megafake_llama_binary"
            
    elif "task2" in dataset_name:
        task = "task2"
        if "full" in dataset_name:
            size = "full"
        else:
            size = "small"
        
        # è§£æ Task2 çš„å­ç±»ä¿¡æ¯
        parts = dataset_name.split("_")
        # æ ¼å¼: task2_full_glm_style_based_fake æˆ– task2_full_llama_content_based_legitimate
        model_source = parts[2]  # glm æˆ– llama
        
        # æ‰¾åˆ°æœ€åä¸€ä¸ªéƒ¨åˆ†ä½œä¸º news_type (fake æˆ– legitimate)
        news_type = parts[-1]
        
        # æå–å­ç±»åç§° (style, content, integration, story)
        # ä» parts[3] å¼€å§‹åˆ°å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼Œå»æ‰ "based"
        subclass_parts = parts[3:-1]
        if "based" in subclass_parts:
            subclass_parts.remove("based")
        subclass = "_".join(subclass_parts)
        
        if model_source == "glm":
            if news_type == "fake":
                data_type = f"glm_{subclass}_based_fake"
            else:
                data_type = f"glm_{subclass}_based_legitimate"
        else:  # llama
            if news_type == "fake":
                data_type = f"llama3_{subclass}_based_fake"
            else:
                data_type = f"llama3_{subclass}_based_legitimate"
            
    elif "task3" in dataset_name:
        task = "task3"
        if "full" in dataset_name:
            size = "full"
        else:
            size = "small"
            
        if "gossip" in dataset_name:
            data_type = "chatglm_gossip_binary"
        else:
            data_type = "chatglm_polifact_binary"
    
    # æ„å»ºä¿å­˜è·¯å¾„
    save_path = f"megafakeTasks/{task}/{size}/result_{data_type}_{model_name}.jsonl"
    return save_path

def get_log_path(model_path, dataset_name):
    """ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    model_name = get_model_name(model_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"inference_{model_name}_{dataset_name}_{timestamp}.log"
    return f"logs/{log_filename}"

def run_inference(model_path, template, dataset_name, save_path, max_new_tokens=10):
    """è¿è¡Œå•ä¸ªæ¨ç†ä»»åŠ¡"""
    cmd = [
        "python", "scripts/vllm_infer.py",
        "--model_name_or_path", model_path,
        "--template", template,
        "--dataset", dataset_name,
        "--save_name", save_path,
        "--max_new_tokens", str(max_new_tokens),
        "--temperature", "0.1",  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
        "--top_p", "0.9",
        "--batch_size", "1024"  
    ]
    
    # ä¸ºæŸäº›æ¨¡å‹æ·»åŠ  trust_remote_code å‚æ•°
    model_name = get_model_name(model_path)
    if "Baichuan" in model_name or "chatglm" in model_name.lower():
        cmd.append("--trust_remote_code")
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_path = get_log_path(model_path, dataset_name)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    
    print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
    
    try:
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as log_file:
            # è®°å½•å‘½ä»¤å’Œæ—¶é—´æˆ³
            log_file.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
            log_file.write("=" * 80 + "\n")
            log_file.flush()
            
            # ä½¿ç”¨ Popen æ¥å®æ—¶è¾“å‡ºæ—¥å¿—
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            # å®æ—¶è¯»å–è¾“å‡ºå¹¶åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
            for line in process.stdout:
                print(line, end='')  # è¾“å‡ºåˆ°æ§åˆ¶å°
                log_file.write(line)  # å†™å…¥æ–‡ä»¶
                log_file.flush()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            log_file.write("=" * 80 + "\n")
            log_file.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"è¿”å›ç : {return_code}\n")
            
            if return_code == 0:
                print(f"âœ… æˆåŠŸå®Œæˆ: {get_model_name(model_path)} + {dataset_name}")
                print(f"   ä¿å­˜è‡³: {save_path}")
                print(f"   æ—¥å¿—è‡³: {log_path}")
                return True
            else:
                print(f"âŒ å¤±è´¥: {get_model_name(model_path)} + {dataset_name}")
                print(f"   è¿”å›ç : {return_code}")
                print(f"   æ—¥å¿—æ–‡ä»¶: {log_path}")
                return False
                
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {get_model_name(model_path)} + {dataset_name}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {e}")
        return False

def check_model_exists(model_path):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(model_path):
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    config_file = os.path.join(model_path, "config.json")
    if not os.path.exists(config_file):
        print(f"âš ï¸  æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹æ¨ç†ä»»åŠ¡")
    print(f"ğŸ“Š æ€»è®¡ {len(MODEL_CONFIGS)} ä¸ªæ¨¡å‹ï¼Œ{len(DATASET_CONFIGS)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ¯ æ€»ä»»åŠ¡æ•°: {len(MODEL_CONFIGS) * len(DATASET_CONFIGS)}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_tasks = len(MODEL_CONFIGS) * len(DATASET_CONFIGS)
    completed_tasks = 0
    failed_tasks = 0
    
    # éå†æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ç»„åˆ
    for model_path, template in MODEL_CONFIGS.items():
        model_name = get_model_name(model_path)
        print(f"\nğŸ”„ å¤„ç†æ¨¡å‹: {model_name} (æ¨¡æ¿: {template})")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not check_model_exists(model_path):
            print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {model_path}")
            failed_tasks += len(DATASET_CONFIGS)
            continue
        
        for dataset_name in DATASET_CONFIGS.keys():
            save_path = get_save_path(model_path, dataset_name)
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»“æœæ–‡ä»¶
            if os.path.exists(save_path):
                print(f"â­ï¸  ç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {save_path}")
                completed_tasks += 1
                continue
            
            # è¿è¡Œæ¨ç†
            print(f"ğŸ¯ å¼€å§‹æ¨ç†: {model_name} + {dataset_name}")
            success = run_inference(model_path, template, dataset_name, save_path)
            
            if success:
                completed_tasks += 1
            else:
                failed_tasks += 1
            
            print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks + failed_tasks}/{total_tasks} "
                  f"(æˆåŠŸ: {completed_tasks}, å¤±è´¥: {failed_tasks})")
    
    print(f"\nğŸ‰ æ¨ç†ä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: {total_tasks} ä¸ªä»»åŠ¡")
    print(f"âœ… æˆåŠŸ: {completed_tasks}")
    print(f"âŒ å¤±è´¥: {failed_tasks}")
    
    if failed_tasks > 0:
        print(f"âš ï¸  æœ‰ {failed_tasks} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main() 