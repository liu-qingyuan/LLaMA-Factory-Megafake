#!/usr/bin/env python3

import os
import subprocess
import json
from pathlib import Path
import datetime
import argparse

REPO_ROOT = Path(__file__).resolve().parent.parent
SA_ROOT = REPO_ROOT / "sensitivity_analysis"
SA_LOG_ROOT = SA_ROOT / "logs" / "infer"
SA_OUTPUT_ROOT = SA_ROOT / "outputs"
LEGACY_OUTPUT_ROOT = REPO_ROOT / "megafakeTasks"
SA_LOG_ROOT.mkdir(parents=True, exist_ok=True)
SA_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
DATASET_INFO_PATH = REPO_ROOT / "data" / "dataset_info.json"
if DATASET_INFO_PATH.exists():
    with open(DATASET_INFO_PATH, "r", encoding="utf-8") as f:
        DATASET_INFO = json.load(f)
else:
    DATASET_INFO = {}

# æ¨¡å‹é…ç½®ï¼šæ¨¡å‹è·¯å¾„ -> æ¨¡æ¿åç§°
MODEL_CONFIGS = {
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": "llama3",
    "/root/autodl-tmp/models/Qwen1.5-7B": "qwen",
    "/root/autodl-tmp/models/chatglm3-6b": "chatglm3",
    "/root/autodl-tmp/models/Mistral-7B-v0.1": "mistral",
    "/root/autodl-tmp/models/Baichuan2-7B-Chat": "baichuan2",
}
MODEL_NAME_MAP = {Path(path).name: path for path in MODEL_CONFIGS.keys()}

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    # "task1_full_glm": "data_table/task1/alpaca_full/alpaca_megafake_glm_binary.json",
    # "task1_full_llama": "data_table/task1/alpaca_full/alpaca_megafake_llama_binary.json",
    # "task1_small_glm": "data_table/task1/small_8k/alpaca_megafake_glm_8k.json",
    # "task1_small_llama": "data_table/task1/small_8k/alpaca_megafake_llama_8k.json",
    # "task3_full_gossip": "data_table/task3/alpaca_full/alpaca_chatglm_gossip_binary.json",
    # "task3_full_polifact": "data_table/task3/alpaca_full/alpaca_chatglm_polifact_binary.json",
    # "task3_small_gossip": "data_table/task3/small_8k/alpaca_chatglm_gossip_8k.json",
    # "task3_small_polifact": "data_table/task3/small_8k/alpaca_chatglm_polifact_8k.json",
    # Mini Test100 åŸºå‡†ï¼ˆ100æ­£/100è´Ÿï¼‰
    "task1_test200_balanced_glm": "data_table/task1/alpaca_test100_balanced/alpaca_megafake_glm_test200_balanced.json",
    # å¤§è§„æ¨¡å®éªŒæ•°æ®é›† (1k - 20k)
    "task1_scale_1000_glm": "data_table/task1/scale_experiment/alpaca_megafake_glm_1000.json",
    "task1_scale_2000_glm": "data_table/task1/scale_experiment/alpaca_megafake_glm_2000.json",
    "task1_scale_5000_glm": "data_table/task1/scale_experiment/alpaca_megafake_glm_5000.json",
    "task1_scale_10000_glm": "data_table/task1/scale_experiment/alpaca_megafake_glm_10000.json",
    "task1_scale_20000_glm": "data_table/task1/scale_experiment/alpaca_megafake_glm_20000.json"
}

# Task3è·¨åŸŸå®éªŒæ˜ å°„ï¼šæ¨ç†æ•°æ®é›† -> è®­ç»ƒæ•°æ®é›†
TASK3_CROSS_DOMAIN_MAPPING = {
    "task3_full_gossip": "task3_small_polifact",    # åœ¨gossipä¸Šæ¨ç†ï¼Œä½¿ç”¨polifactè®­ç»ƒçš„æ¨¡å‹
    "task3_full_polifact": "task3_small_gossip",    # åœ¨polifactä¸Šæ¨ç†ï¼Œä½¿ç”¨gossipè®­ç»ƒçš„æ¨¡å‹
    "task3_small_gossip": "task3_small_polifact",   # åœ¨small gossipä¸Šæ¨ç†ï¼Œä½¿ç”¨small polifactè®­ç»ƒçš„æ¨¡å‹
    "task3_small_polifact": "task3_small_gossip"    # åœ¨small polifactä¸Šæ¨ç†ï¼Œä½¿ç”¨small gossipè®­ç»ƒçš„æ¨¡å‹
}

def get_model_name(model_path):
    """ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°"""
    return Path(model_path).name

def get_lora_adapter_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”ŸæˆLoRAé€‚é…å™¨è·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡ç±»å‹
    if "task1" in dataset_name:
        task = "task1"
        # Task1: æ‰€æœ‰LoRAæ¨¡å‹éƒ½æ˜¯ç”¨smallæ•°æ®é›†è®­ç»ƒçš„
        if "full" in dataset_name:
            train_dataset = dataset_name.replace("full", "small")
        else:
            train_dataset = dataset_name
    elif "task2" in dataset_name:
        task = "task2"
        # Task2: æ‰€æœ‰LoRAæ¨¡å‹éƒ½æ˜¯ç”¨smallæ•°æ®é›†è®­ç»ƒçš„
        if "full" in dataset_name:
            train_dataset = dataset_name.replace("full", "small")
        else:
            train_dataset = dataset_name
    elif "task3" in dataset_name:
        task = "task3"
        # Task3: è·¨åŸŸå®éªŒï¼Œä½¿ç”¨æ˜ å°„è¡¨
        train_dataset = TASK3_CROSS_DOMAIN_MAPPING.get(dataset_name, dataset_name)
    
    # adapter_path = REPO_ROOT / f"megafakeTasks/{task}/{train_dataset}/{model_name}/lora/sft"
    adapter_path = SA_OUTPUT_ROOT / task / train_dataset / model_name / "lora" / "sft"
    legacy_path = LEGACY_OUTPUT_ROOT / task / train_dataset / model_name / "lora" / "sft"
    if not adapter_path.exists() and legacy_path.exists():
        return str(legacy_path)
    return str(adapter_path)

def get_save_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆä¿å­˜è·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡å’Œç±»å‹
    if "task1" in dataset_name:
        task = "task1"
        if "full" in dataset_name:
            size = "full"
        elif "test100" in dataset_name:
            size = "test100"
        elif "test200" in dataset_name:
            size = "test200_balanced"
        elif "scale" in dataset_name:
            # task1_scale_1000_glm -> scale_1000
            parts = dataset_name.split('_')
            try:
                idx = parts.index("scale")
                scale_val = parts[idx+1]
                size = f"scale_{scale_val}"
            except (ValueError, IndexError):
                size = "scale_unknown"
        else:
            size = "small"
        
        if "glm" in dataset_name:
            data_type = "megafake_glm_binary"
        else:
            data_type = "megafake_llama_binary"
            
    elif "task3" in dataset_name:
        task = "task3"
        if "full" in dataset_name:
            size = "full"
        else:
            size = "small"
            
        if "gossip" in dataset_name:
            data_type = "chatglm_gossip_binary"
        else:
            data_type = "chatglm_polifact_binary"
    
    # ä¿®æ”¹ä¿å­˜è·¯å¾„ï¼ŒåŒ…å«LoRAæ ‡è¯†
    # å¯¹äºtask3è·¨åŸŸå®éªŒï¼Œéœ€è¦åœ¨æ–‡ä»¶åä¸­ä½“ç°è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    if task == "task3" and dataset_name in TASK3_CROSS_DOMAIN_MAPPING:
        train_dataset = TASK3_CROSS_DOMAIN_MAPPING[dataset_name]
        train_type = "polifact" if "polifact" in train_dataset else "gossip"
        save_path = SA_OUTPUT_ROOT / task / size / f"result_{dataset_name}_{model_name}_LoRA_trained_on_{train_type}.jsonl"
    else:
        # save_path = REPO_ROOT / f"megafakeTasks/{task}/{size}/result_{dataset_name}_{model_name}_LoRA.jsonl"
        save_path = SA_OUTPUT_ROOT / task / size / f"result_{dataset_name}_{model_name}_LoRA.jsonl"
    return str(save_path)

def get_log_path(model_path, dataset_name):
    """ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    model_name = get_model_name(model_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"inference_LoRA_{model_name}_{dataset_name}_{timestamp}.log"
    # return f"logs/{log_filename}"
    return SA_LOG_ROOT / log_filename

def run_inference(model_path, template, dataset_name, save_path, max_new_tokens=10):
    """è¿è¡Œå•ä¸ªæ¨ç†ä»»åŠ¡"""
    # è·å–LoRAé€‚é…å™¨è·¯å¾„
    adapter_path = get_lora_adapter_path(model_path, dataset_name)
    adapter_config = Path(adapter_path) / "adapter_config.json"
    if not adapter_config.exists():
        print(f"âŒ LoRAé€‚é…å™¨æœªæ‰¾åˆ°ï¼Œè·³è¿‡: {adapter_path}")
        return False, None
    
    model_name = get_model_name(model_path)
    
    cmd = [
        "python", "scripts/vllm_infer.py",
        "--model_name_or_path", model_path,
        "--adapter_name_or_path", adapter_path,
        "--template", template,
        "--dataset", dataset_name,
        "--save_name", save_path,
        "--max_new_tokens", str(max_new_tokens),
        "--temperature", "0.1",  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
        "--top_p", "0.9",
        "--batch_size", "1024"  
    ]
    
    # ä¸ºæŸäº›æ¨¡å‹æ·»åŠ  trust_remote_code å‚æ•°
    model_name = get_model_name(model_path)
    if "Baichuan" in model_name or "chatglm" in model_name.lower():
        cmd.append("--trust_remote_code")
    
    # Mistral specific config
    if "Mistral" in model_name:
        # é’ˆå¯¹æŸäº› VLLM ç‰ˆæœ¬å¯èƒ½éœ€è¦æ˜¾å¼è®¾ç½® rotary parametersï¼Œä½†å¦‚æœæ˜¯ v0.1 ä¸” vllm è¾ƒæ–°ï¼Œé€šå¸¸ä¸éœ€è¦ã€‚
        # å¦‚æœå¿…é¡»è¡¥é½ partial_rotary_factorï¼Œå¯ä»¥å°è¯•å¦‚ä¸‹ï¼š
        # cmd.extend(["--vllm_config", '{"partial_rotary_factor": 1.0}'])
        pass
    
    # æ£€æŸ¥LoRAé€‚é…å™¨æ˜¯å¦å­˜åœ¨
    if not os.path.exists(adapter_path):
        print(f"âŒ LoRAé€‚é…å™¨ä¸å­˜åœ¨: {adapter_path}")
        return False
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_path = get_log_path(model_path, dataset_name)
    Path(log_path).parent.mkdir(parents=True, exist_ok=True)
    
    print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print(f"LoRAé€‚é…å™¨: {adapter_path}")
    print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
    
    try:
        # æ„å»ºåŒ…å«ç¯å¢ƒé…ç½®çš„å‘½ä»¤
        env_cmd = [
            "bash", "-c", 
            "export HF_ENDPOINT=https://hf-mirror.com && "
            "source /etc/network_turbo 2>/dev/null || true && "
            f"{' '.join(cmd)}"
        ]
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as log_file:
            # è®°å½•å‘½ä»¤å’Œæ—¶é—´æˆ³
            log_file.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
            log_file.write(f"LoRAé€‚é…å™¨: {adapter_path}\n")
            log_file.write("=" * 80 + "\n")
            log_file.flush()
            
            # ä½¿ç”¨ Popen æ¥å®æ—¶è¾“å‡ºæ—¥å¿—
            process = subprocess.Popen(
                env_cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            # å®æ—¶è¯»å–è¾“å‡ºå¹¶åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
            for line in process.stdout:
                print(line, end='')  # è¾“å‡ºåˆ°æ§åˆ¶å°
                log_file.write(line)  # å†™å…¥æ–‡ä»¶
                log_file.flush()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            log_file.write("=" * 80 + "\n")
            log_file.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"è¿”å›ç : {return_code}\n")
            
            if return_code == 0:
                print(f"âœ… æˆåŠŸå®Œæˆ: {model_name} + {dataset_name}")
                print(f"   ä¿å­˜è‡³: {Path(save_path).resolve()}")
                print(f"   æ—¥å¿—è‡³: {log_path.resolve()}")
                next_cmd = (
                    "python scripts/analyze_predictions.py "
                    f"--file {Path(save_path).resolve()} --output sensitivity_analysis/results/{Path(save_path).stem}_metrics.csv"
                )
                print(f"ğŸ‘‰ ä¸‹ä¸€æ­¥: {next_cmd}")
                return True, str(log_path.resolve())
            else:
                print(f"âŒ å¤±è´¥: {model_name} + {dataset_name}")
                print(f"   è¿”å›ç : {return_code}")
                print(f"   æ—¥å¿—æ–‡ä»¶: {log_path.resolve()}")
                return False, str(log_path.resolve())
                
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {model_name} + {dataset_name}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {e}")
        return False, None

def check_model_exists(model_path):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(model_path):
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    config_file = os.path.join(model_path, "config.json")
    if not os.path.exists(config_file):
        print(f"âš ï¸  æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    return True

def resolve_dataset_file(dataset_key):
    """è§£ææ•°æ®é›†æ–‡ä»¶"""
    info = DATASET_INFO.get(dataset_key)
    if not info:
        return None
    file_name = info.get("file_name")
    if not file_name:
        return None
    return REPO_ROOT / "data" / file_name

def dry_run_check(model_path, dataset_key):
    """LoRA æ¨ç† dry-run æ£€æŸ¥"""
    dataset_file = resolve_dataset_file(dataset_key)
    dataset_ready = dataset_file.exists() if dataset_file else False
    adapter_path = get_lora_adapter_path(model_path, dataset_key)
    adapter_ready = Path(adapter_path).exists()
    save_path = get_save_path(model_path, dataset_key)
    save_dir = Path(save_path).parent
    print(f"\n[Dry-Run] æ¨¡å‹: {model_path}")
    print(f"[Dry-Run] æ•°æ®é›†: {dataset_key}")
    if dataset_file:
        print(f"[Dry-Run] æ•°æ®æ–‡ä»¶: {dataset_file} {'âœ…' if dataset_ready else 'âŒ'}")
    else:
        print(f"[Dry-Run] æ•°æ®æ–‡ä»¶: æœªåœ¨ dataset_info.json ä¸­ç™»è®° âŒ")
    print(f"[Dry-Run] LoRA é€‚é…å™¨: {adapter_path} {'âœ…' if adapter_ready else 'âŒ'}")
    print(f"[Dry-Run] ç»“æœå°†å†™å…¥: {Path(save_path).resolve()}")
    try:
        save_dir.mkdir(parents=True, exist_ok=True)
    except OSError as exc:
        print(f"[Dry-Run] åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {exc}")
        return False
    return dataset_ready and adapter_ready and check_model_exists(model_path)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æ‰¹é‡è¿è¡Œ LoRA æ¨ç†ï¼ˆæ”¯æŒ Dry-Runï¼‰")
    parser.add_argument("--models", nargs="+", help="æŒ‡å®šæ¨¡å‹åç§°ï¼ˆå¦‚ Qwen1.5-7Bï¼‰æˆ–ç»å¯¹è·¯å¾„")
    parser.add_argument("--datasets", nargs="+", choices=list(DATASET_CONFIGS.keys()),
                        help="æŒ‡å®šæ•°æ®é›†é”®ï¼ˆé»˜è®¤å…¨é‡ï¼‰")
    parser.add_argument("--limit", type=int, help="é™åˆ¶ä»»åŠ¡æ•°é‡")
    parser.add_argument("--dry-run", action="store_true", help="ä»…æ£€æŸ¥ä¾èµ–ï¼Œä¸å®é™…æ¨ç†")
    parser.add_argument("--skip-existing", action="store_true", help="ç»“æœå­˜åœ¨æ—¶è·³è¿‡")
    parser.add_argument("--max-new-tokens", type=int, help="è¦†ç›–é»˜è®¤ max_new_tokens")
    parser.add_argument("--include-large-models", action="store_true", help="å…è®¸è¿è¡Œ 70B+ å¤§æ¨¡å‹")
    return parser.parse_args()

def select_models(model_filters, include_large=False):
    """ç­›é€‰æ¨¡å‹"""
    if not model_filters:
        items = list(MODEL_CONFIGS.items())
    else:
        items = []
        for item in model_filters:
            if item in MODEL_CONFIGS:
                items.append((item, MODEL_CONFIGS[item]))
                continue
            if item in MODEL_NAME_MAP:
                path = MODEL_NAME_MAP[item]
                items.append((path, MODEL_CONFIGS[path]))
                continue
            resolved = Path(item).expanduser()
            if str(resolved) in MODEL_CONFIGS:
                items.append((str(resolved), MODEL_CONFIGS[str(resolved)]))
            else:
                print(f"âš ï¸  æœªè¯†åˆ«çš„æ¨¡å‹: {item}")
    
    # è¿‡æ»¤å¤§æ¨¡å‹
    final_selection = []
    for path, template in items:
        if "72B" in str(path) or "70B" in str(path):
            if not include_large:
                print(f"âš ï¸  è·³è¿‡å¤§æ¨¡å‹ (éœ€ --include-large-models): {path}")
                continue
            else:
                print(f"âš ï¸  åŒ…å«å¤§æ¨¡å‹ (OOMé£é™©): {path}")
        final_selection.append((path, template))
        
    return final_selection

def select_datasets(dataset_filters):
    """ç­›é€‰æ•°æ®é›†"""
    if not dataset_filters:
        return list(DATASET_CONFIGS.keys())
    selected = []
    for key in dataset_filters:
        if key in DATASET_CONFIGS:
            selected.append(key)
        else:
            print(f"âš ï¸  æœªè¯†åˆ«çš„æ•°æ®é›†: {key}")
    return selected

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    selected_models = select_models(args.models, args.include_large_models)
    selected_datasets = select_datasets(args.datasets)
    total_tasks = len(selected_models) * len(selected_datasets)
    if args.limit:
        total_tasks = min(total_tasks, args.limit)
    print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹LoRAæ¨ç†ä»»åŠ¡")
    print(f"ğŸ“Š é€‰ä¸­ {len(selected_models)} ä¸ªæ¨¡å‹ï¼Œ{len(selected_datasets)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ¯ è®¡åˆ’ä»»åŠ¡æ•°: {total_tasks}")

    if args.dry_run:
        issues = False
        processed = 0
        for model_path, template in selected_models:
            for dataset_name in selected_datasets:
                if args.limit and processed >= args.limit:
                    break
                processed += 1
                ok = dry_run_check(model_path, dataset_name)
                if not ok:
                    issues = True
            if args.limit and processed >= args.limit:
                break
        if issues:
            print("\nâš ï¸  Dry-Run æ£€æµ‹åˆ°é—®é¢˜ï¼Œè¯·ä¿®å¤åå†è¿è¡Œ")
            return
        print("\nâœ… Dry-Run æ£€æŸ¥é€šè¿‡ï¼Œå¯å®‰å…¨å¯åŠ¨æ¨ç†")
        return

    completed_tasks = 0
    failed_tasks = 0
    processed_tasks = 0
    artifact_records = []
    for model_path, template in selected_models:
        model_name = get_model_name(model_path)
        print(f"\nğŸ”„ å¤„ç†æ¨¡å‹: {model_name} (æ¨¡æ¿: {template})")
        if not check_model_exists(model_path):
            print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {model_path}")
            failed_tasks += len(selected_datasets)
            continue
        for dataset_name in selected_datasets:
            if args.limit and processed_tasks >= args.limit:
                break
            processed_tasks += 1
            save_path = get_save_path(model_path, dataset_name)
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            if args.skip_existing and Path(save_path).exists():
                print(f"â­ï¸  ç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {save_path}")
                completed_tasks += 1
                artifact_records.append({
                    "model": model_name,
                    "dataset": dataset_name,
                    "result": save_path,
                    "log": None
                })
                continue
            print(f"ğŸ¯ å¼€å§‹LoRAæ¨ç†: {model_name} + {dataset_name}")
            success, log_path = run_inference(
                model_path,
                template,
                dataset_name,
                save_path,
                max_new_tokens=args.max_new_tokens if args.max_new_tokens else 10
            )
            if success:
                completed_tasks += 1
                artifact_records.append({
                    "model": model_name,
                    "dataset": dataset_name,
                    "result": save_path,
                    "log": log_path
                })
            else:
                failed_tasks += 1
            print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks + failed_tasks}/{total_tasks} "
                  f"(æˆåŠŸ: {completed_tasks}, å¤±è´¥: {failed_tasks})")
        if args.limit and processed_tasks >= args.limit:
            break

    print(f"\nğŸ‰ LoRAæ¨ç†ä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: {processed_tasks} ä¸ªä»»åŠ¡")
    print(f"âœ… æˆåŠŸ: {completed_tasks}")
    print(f"âŒ å¤±è´¥: {failed_tasks}")
    if failed_tasks > 0:
        print(f"âš ï¸  æœ‰ {failed_tasks} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
    if artifact_records:
        print("\nğŸ“¦ æ¨ç†äº§ç‰©:")
        for record in artifact_records:
            print(f"  - {record['model']} @ {record['dataset']}")
            print(f"    ç»“æœ: {record['result']}")
            if record["log"]:
                print(f"    æ—¥å¿—: {record['log']}")
        recommend_cmd = (
            "python scripts/analyze_predictions.py "
            "--dir sensitivity_analysis/outputs "
            "--output sensitivity_analysis/results/mini_test100_metrics.csv"
        )
        print(f"\nğŸ”œ æ¨èä¸‹ä¸€æ­¥: {recommend_cmd}")
        print(f"ğŸ“ æ¨ç†ç»“æœæ ¹ç›®å½•: {SA_OUTPUT_ROOT}")

if __name__ == "__main__":
    main() 
