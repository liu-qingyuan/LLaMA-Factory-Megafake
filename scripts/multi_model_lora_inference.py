#!/usr/bin/env python3

import os
import subprocess
import json
from pathlib import Path
import datetime

# æ¨¡å‹é…ç½®ï¼šæ¨¡å‹è·¯å¾„ -> æ¨¡æ¿åç§°
MODEL_CONFIGS = {
    # "/root/autodl-tmp/models/Baichuan2-7B-Chat": "baichuan2",
    "/root/autodl-tmp/models/chatglm3-6b": "chatglm3", 
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": "llama3",
    # "/root/autodl-tmp/models/Mistral-7B-v0.1": "mistral",
    "/root/autodl-tmp/models/Qwen1.5-7B": "qwen"
}

# æ•°æ®é›†é…ç½®
DATASET_CONFIGS = {
    "task1_full_glm": "data_table/task1/alpaca_full/alpaca_megafake_glm_binary.json",
    "task1_full_llama": "data_table/task1/alpaca_full/alpaca_megafake_llama_binary.json", 
    # "task1_small_glm": "data_table/task1/small_8k/alpaca_megafake_glm_8k.json",
    # "task1_small_llama": "data_table/task1/small_8k/alpaca_megafake_llama_8k.json",
    "task3_full_gossip": "data_table/task3/alpaca_full/alpaca_chatglm_gossip_binary.json",
    "task3_full_polifact": "data_table/task3/alpaca_full/alpaca_chatglm_polifact_binary.json",
    # "task3_small_gossip": "data_table/task3/small_8k/alpaca_chatglm_gossip_8k.json",
    # "task3_small_polifact": "data_table/task3/small_8k/alpaca_chatglm_polifact_8k.json"
}

# Task3è·¨åŸŸå®éªŒæ˜ å°„ï¼šæ¨ç†æ•°æ®é›† -> è®­ç»ƒæ•°æ®é›†
TASK3_CROSS_DOMAIN_MAPPING = {
    "task3_full_gossip": "task3_small_polifact",    # åœ¨gossipä¸Šæ¨ç†ï¼Œä½¿ç”¨polifactè®­ç»ƒçš„æ¨¡å‹
    "task3_full_polifact": "task3_small_gossip",    # åœ¨polifactä¸Šæ¨ç†ï¼Œä½¿ç”¨gossipè®­ç»ƒçš„æ¨¡å‹
    "task3_small_gossip": "task3_small_polifact",   # åœ¨small gossipä¸Šæ¨ç†ï¼Œä½¿ç”¨small polifactè®­ç»ƒçš„æ¨¡å‹
    "task3_small_polifact": "task3_small_gossip"    # åœ¨small polifactä¸Šæ¨ç†ï¼Œä½¿ç”¨small gossipè®­ç»ƒçš„æ¨¡å‹
}

def get_model_name(model_path):
    """ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°"""
    return Path(model_path).name

def get_lora_adapter_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”ŸæˆLoRAé€‚é…å™¨è·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡ç±»å‹
    if "task1" in dataset_name:
        task = "task1"
        # Task1: æ‰€æœ‰LoRAæ¨¡å‹éƒ½æ˜¯ç”¨smallæ•°æ®é›†è®­ç»ƒçš„
        if "full" in dataset_name:
            train_dataset = dataset_name.replace("full", "small")
        else:
            train_dataset = dataset_name
    elif "task2" in dataset_name:
        task = "task2"
        # Task2: æ‰€æœ‰LoRAæ¨¡å‹éƒ½æ˜¯ç”¨smallæ•°æ®é›†è®­ç»ƒçš„
        if "full" in dataset_name:
            train_dataset = dataset_name.replace("full", "small")
        else:
            train_dataset = dataset_name
    elif "task3" in dataset_name:
        task = "task3"
        # Task3: è·¨åŸŸå®éªŒï¼Œä½¿ç”¨æ˜ å°„è¡¨
        train_dataset = TASK3_CROSS_DOMAIN_MAPPING.get(dataset_name, dataset_name)
    
    adapter_path = f"/root/autodl-tmp/LLaMA-Factory-Megafake2/megafakeTasks/{task}/{train_dataset}/{model_name}/lora/sft"
    return adapter_path

def get_save_path(model_path, dataset_name):
    """æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆä¿å­˜è·¯å¾„"""
    model_name = get_model_name(model_path)
    
    # æ ¹æ®æ•°æ®é›†åç§°ç¡®å®šä»»åŠ¡å’Œç±»å‹
    if "task1" in dataset_name:
        task = "task1"
        if "full" in dataset_name:
            size = "full"
        else:
            size = "small"
        
        if "glm" in dataset_name:
            data_type = "megafake_glm_binary"
        else:
            data_type = "megafake_llama_binary"
            
    elif "task3" in dataset_name:
        task = "task3"
        if "full" in dataset_name:
            size = "full"
        else:
            size = "small"
            
        if "gossip" in dataset_name:
            data_type = "chatglm_gossip_binary"
        else:
            data_type = "chatglm_polifact_binary"
    
    # ä¿®æ”¹ä¿å­˜è·¯å¾„ï¼ŒåŒ…å«LoRAæ ‡è¯†
    # å¯¹äºtask3è·¨åŸŸå®éªŒï¼Œéœ€è¦åœ¨æ–‡ä»¶åä¸­ä½“ç°è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    if task == "task3" and dataset_name in TASK3_CROSS_DOMAIN_MAPPING:
        train_dataset = TASK3_CROSS_DOMAIN_MAPPING[dataset_name]
        train_type = "polifact" if "polifact" in train_dataset else "gossip"
        save_path = f"megafakeTasks/{task}/{size}/result_{dataset_name}_{model_name}_LoRA_trained_on_{train_type}.jsonl"
    else:
        save_path = f"megafakeTasks/{task}/{size}/result_{dataset_name}_{model_name}_LoRA.jsonl"
    return save_path

def get_log_path(model_path, dataset_name):
    """ç”Ÿæˆæ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    model_name = get_model_name(model_path)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"inference_LoRA_{model_name}_{dataset_name}_{timestamp}.log"
    return f"logs/{log_filename}"

def run_inference(model_path, template, dataset_name, save_path, max_new_tokens=10):
    """è¿è¡Œå•ä¸ªæ¨ç†ä»»åŠ¡"""
    # è·å–LoRAé€‚é…å™¨è·¯å¾„
    adapter_path = get_lora_adapter_path(model_path, dataset_name)
    
    cmd = [
        "python", "scripts/vllm_infer.py",
        "--model_name_or_path", model_path,
        "--adapter_name_or_path", adapter_path,
        "--template", template,
        "--dataset", dataset_name,
        "--save_name", save_path,
        "--max_new_tokens", str(max_new_tokens),
        "--temperature", "0.1",  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
        "--top_p", "0.9",
        "--batch_size", "1024"  
    ]
    
    # ä¸ºæŸäº›æ¨¡å‹æ·»åŠ  trust_remote_code å‚æ•°
    model_name = get_model_name(model_path)
    if "Baichuan" in model_name or "chatglm" in model_name.lower():
        cmd.append("--trust_remote_code")
    
    # æ£€æŸ¥LoRAé€‚é…å™¨æ˜¯å¦å­˜åœ¨
    if not os.path.exists(adapter_path):
        print(f"âŒ LoRAé€‚é…å™¨ä¸å­˜åœ¨: {adapter_path}")
        return False
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_path = get_log_path(model_path, dataset_name)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    
    print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print(f"LoRAé€‚é…å™¨: {adapter_path}")
    print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
    
    try:
        # æ„å»ºåŒ…å«ç¯å¢ƒé…ç½®çš„å‘½ä»¤
        env_cmd = [
            "bash", "-c", 
            "export HF_ENDPOINT=https://hf-mirror.com && "
            "source /etc/network_turbo 2>/dev/null || true && "
            f"{' '.join(cmd)}"
        ]
        
        # æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        with open(log_path, 'w', encoding='utf-8') as log_file:
            # è®°å½•å‘½ä»¤å’Œæ—¶é—´æˆ³
            log_file.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
            log_file.write(f"LoRAé€‚é…å™¨: {adapter_path}\n")
            log_file.write("=" * 80 + "\n")
            log_file.flush()
            
            # ä½¿ç”¨ Popen æ¥å®æ—¶è¾“å‡ºæ—¥å¿—
            process = subprocess.Popen(
                env_cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            # å®æ—¶è¯»å–è¾“å‡ºå¹¶åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
            for line in process.stdout:
                print(line, end='')  # è¾“å‡ºåˆ°æ§åˆ¶å°
                log_file.write(line)  # å†™å…¥æ–‡ä»¶
                log_file.flush()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait()
            
            # è®°å½•ç»“æŸæ—¶é—´
            log_file.write("=" * 80 + "\n")
            log_file.write(f"ç»“æŸæ—¶é—´: {datetime.datetime.now()}\n")
            log_file.write(f"è¿”å›ç : {return_code}\n")
            
            if return_code == 0:
                print(f"âœ… æˆåŠŸå®Œæˆ: {get_model_name(model_path)} + {dataset_name}")
                print(f"   ä¿å­˜è‡³: {save_path}")
                print(f"   æ—¥å¿—è‡³: {log_path}")
                return True
            else:
                print(f"âŒ å¤±è´¥: {get_model_name(model_path)} + {dataset_name}")
                print(f"   è¿”å›ç : {return_code}")
                print(f"   æ—¥å¿—æ–‡ä»¶: {log_path}")
                return False
                
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {get_model_name(model_path)} + {dataset_name}")
        print(f"   å¼‚å¸¸ä¿¡æ¯: {e}")
        return False

def check_model_exists(model_path):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    if not os.path.exists(model_path):
        return False
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶
    config_file = os.path.join(model_path, "config.json")
    if not os.path.exists(config_file):
        print(f"âš ï¸  æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹LoRAæ¨ç†ä»»åŠ¡")
    print(f"ğŸ“Š æ€»è®¡ {len(MODEL_CONFIGS)} ä¸ªæ¨¡å‹ï¼Œ{len(DATASET_CONFIGS)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ¯ æ€»ä»»åŠ¡æ•°: {len(MODEL_CONFIGS) * len(DATASET_CONFIGS)}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_tasks = len(MODEL_CONFIGS) * len(DATASET_CONFIGS)
    completed_tasks = 0
    failed_tasks = 0
    
    # éå†æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ç»„åˆ
    for model_path, template in MODEL_CONFIGS.items():
        model_name = get_model_name(model_path)
        print(f"\nğŸ”„ å¤„ç†æ¨¡å‹: {model_name} (æ¨¡æ¿: {template})")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not check_model_exists(model_path):
            print(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡: {model_path}")
            failed_tasks += len(DATASET_CONFIGS)
            continue
        
        for dataset_name in DATASET_CONFIGS.keys():
            save_path = get_save_path(model_path, dataset_name)
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»“æœæ–‡ä»¶
            if os.path.exists(save_path):
                print(f"â­ï¸  ç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {save_path}")
                completed_tasks += 1
                continue
            
            # è¿è¡ŒLoRAæ¨ç†
            print(f"ğŸ¯ å¼€å§‹LoRAæ¨ç†: {model_name} + {dataset_name}")
            success = run_inference(model_path, template, dataset_name, save_path)
            
            if success:
                completed_tasks += 1
            else:
                failed_tasks += 1
            
            print(f"ğŸ“ˆ è¿›åº¦: {completed_tasks + failed_tasks}/{total_tasks} "
                  f"(æˆåŠŸ: {completed_tasks}, å¤±è´¥: {failed_tasks})")
    
    print(f"\nğŸ‰ LoRAæ¨ç†ä»»åŠ¡å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: {total_tasks} ä¸ªä»»åŠ¡")
    print(f"âœ… æˆåŠŸ: {completed_tasks}")
    print(f"âŒ å¤±è´¥: {failed_tasks}")
    
    if failed_tasks > 0:
        print(f"âš ï¸  æœ‰ {failed_tasks} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main() 