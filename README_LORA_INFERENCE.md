# LoRAæ¨¡å‹æ‰¹é‡æ¨ç†è„šæœ¬ä½¿ç”¨è¯´æ˜

## æ¦‚è¿°

`multi_model_lora_inference.py` æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡æ¨ç†è®­ç»ƒå¥½çš„LoRAæ¨¡å‹çš„è„šæœ¬ã€‚è¯¥è„šæœ¬åŸºäºåŸå§‹çš„å¤šæ¨¡å‹æ¨ç†è„šæœ¬ä¿®æ”¹ï¼Œä¸“é—¨ç”¨äºä½¿ç”¨LoRAé€‚é…å™¨è¿›è¡Œæ¨ç†ã€‚

## ä¸»è¦ä¿®æ”¹

ç›¸æ¯”åŸå§‹çš„æ¨ç†è„šæœ¬ï¼Œä¸»è¦ä¿®æ”¹åŒ…æ‹¬ï¼š

1. **æ·»åŠ LoRAé€‚é…å™¨è·¯å¾„**: è‡ªåŠ¨æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†ç”Ÿæˆå¯¹åº”çš„LoRAé€‚é…å™¨è·¯å¾„
2. **ä¿®æ”¹æ¨ç†å‘½ä»¤**: æ·»åŠ  `--adapter_name_or_path` å‚æ•°
3. **é€‚é…å™¨å­˜åœ¨æ€§æ£€æŸ¥**: åœ¨æ¨ç†å‰æ£€æŸ¥LoRAé€‚é…å™¨æ˜¯å¦å­˜åœ¨
4. **è¾“å‡ºè·¯å¾„ä¼˜åŒ–**: åœ¨ç»“æœæ–‡ä»¶åä¸­åŒ…å«LoRAæ ‡è¯†
5. **ç¯å¢ƒé…ç½®**: æ·»åŠ HuggingFaceé•œåƒå’Œå­¦æœ¯åŠ é€Ÿé…ç½®

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¯ **LoRAæ¨ç†**: ä½¿ç”¨è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨è¿›è¡Œæ¨ç†
- ğŸ” **è‡ªåŠ¨è·¯å¾„åŒ¹é…**: è‡ªåŠ¨åŒ¹é…æ¨¡å‹å’Œæ•°æ®é›†å¯¹åº”çš„LoRAé€‚é…å™¨
- âœ… **å­˜åœ¨æ€§éªŒè¯**: æ¨ç†å‰éªŒè¯LoRAé€‚é…å™¨æ˜¯å¦å­˜åœ¨
- ğŸ“ **è¯¦ç»†æ—¥å¿—**: è®°å½•LoRAé€‚é…å™¨è·¯å¾„å’Œæ¨ç†è¿‡ç¨‹
- ğŸš€ **åŠ é€Ÿé…ç½®**: è‡ªåŠ¨é…ç½®HuggingFaceé•œåƒå’Œç½‘ç»œåŠ é€Ÿ

## LoRAé€‚é…å™¨è·¯å¾„è§„åˆ™

è„šæœ¬ä¼šæ ¹æ®ä»¥ä¸‹è§„åˆ™ç”ŸæˆLoRAé€‚é…å™¨è·¯å¾„ï¼š

```
/root/autodl-tmp/LLaMA-Factory-Megafake2/megafakeTasks/{task}/{dataset}/{model_name}/lora/sft
```

ä¾‹å¦‚ï¼š
- æ¨¡å‹: `Meta-Llama-3.1-8B-Instruct`
- æ•°æ®é›†: `task1_full_glm`
- é€‚é…å™¨è·¯å¾„: `/root/autodl-tmp/LLaMA-Factory-Megafake2/megafakeTasks/task1/task1_full_glm/Meta-Llama-3.1-8B-Instruct/lora/sft`

## æ¨ç†å‘½ä»¤æ ¼å¼

ç”Ÿæˆçš„æ¨ç†å‘½ä»¤æ ¼å¼ï¼š

```bash
export HF_ENDPOINT=https://hf-mirror.com && \
source /etc/network_turbo && \
python scripts/vllm_infer.py \
    --model_name_or_path /root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct \
    --adapter_name_or_path /root/autodl-tmp/LLaMA-Factory-Megafake2/megafakeTasks/task1/task1_full_glm/Meta-Llama-3.1-8B-Instruct/lora/sft \
    --template llama3 \
    --dataset task1_full_glm \
    --save_name megafakeTasks/task1/full/result_task1_full_glm_Meta-Llama-3.1-8B-Instruct_LoRA.jsonl \
    --max_new_tokens 10 \
    --temperature 0.1 \
    --top_p 0.9 \
    --batch_size 1024
```

## è¾“å‡ºæ–‡ä»¶å‘½å

ç»“æœæ–‡ä»¶å‘½åæ ¼å¼ï¼š
```
result_{dataset_name}_{model_name}_LoRA.jsonl
```

ä¾‹å¦‚ï¼š
- `result_task1_full_glm_Meta-Llama-3.1-8B-Instruct_LoRA.jsonl`
- `result_task3_full_gossip_chatglm3-6b_LoRA.jsonl`

## ä½¿ç”¨æ–¹æ³•

### 1. ç¡®ä¿LoRAæ¨¡å‹å·²è®­ç»ƒ

åœ¨è¿è¡Œæ¨ç†å‰ï¼Œç¡®ä¿å·²ç»ä½¿ç”¨ `multi_model_lora_train.py` è®­ç»ƒäº†å¯¹åº”çš„LoRAæ¨¡å‹ã€‚

### 2. è¿è¡Œæ¨ç†è„šæœ¬

```bash
cd /root/autodl-tmp/LLaMA-Factory-Megafake2
python scripts/multi_model_lora_inference.py
```

### 3. æ£€æŸ¥ç»“æœ

æ¨ç†ç»“æœä¿å­˜åœ¨ `megafakeTasks/` ç›®å½•ä¸‹å¯¹åº”çš„ä»»åŠ¡æ–‡ä»¶å¤¹ä¸­ã€‚

## çŠ¶æ€æ£€æŸ¥

è„šæœ¬ä¼šè‡ªåŠ¨è¿›è¡Œä»¥ä¸‹æ£€æŸ¥ï¼š

1. **åŸºç¡€æ¨¡å‹å­˜åœ¨æ€§**: æ£€æŸ¥åŸå§‹æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
2. **LoRAé€‚é…å™¨å­˜åœ¨æ€§**: æ£€æŸ¥å¯¹åº”çš„LoRAé€‚é…å™¨æ˜¯å¦å­˜åœ¨
3. **ç»“æœæ–‡ä»¶å­˜åœ¨æ€§**: å¦‚æœç»“æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è¯¥ä»»åŠ¡

## æ—¥å¿—æ–‡ä»¶

æ—¥å¿—æ–‡ä»¶å‘½åæ ¼å¼ï¼š
```
logs/inference_LoRA_{model_name}_{dataset_name}_{timestamp}.log
```

æ—¥å¿—å†…å®¹åŒ…æ‹¬ï¼š
- å¼€å§‹æ—¶é—´
- æ‰§è¡Œå‘½ä»¤
- LoRAé€‚é…å™¨è·¯å¾„
- æ¨ç†è¿‡ç¨‹è¾“å‡º
- ç»“æŸæ—¶é—´å’Œè¿”å›ç 

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```python
MODEL_CONFIGS = {
    "/root/autodl-tmp/models/Baichuan2-7B-Chat": "baichuan2",
    "/root/autodl-tmp/models/chatglm3-6b": "chatglm3", 
    "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct": "llama3",
    "/root/autodl-tmp/models/Mistral-7B-v0.1": "mistral",
    "/root/autodl-tmp/models/Qwen1.5-7B": "qwen"
}
```

### æ•°æ®é›†é…ç½®

```python
DATASET_CONFIGS = {
    "task1_full_glm": "data_table/task1/alpaca_full/alpaca_megafake_glm_binary.json",
    "task1_full_llama": "data_table/task1/alpaca_full/alpaca_megafake_llama_binary.json", 
    "task3_full_gossip": "data_table/task3/alpaca_full/alpaca_chatglm_gossip_binary.json",
    "task3_full_polifact": "data_table/task3/alpaca_full/alpaca_chatglm_polifact_binary.json"
}
```

## é”™è¯¯å¤„ç†

- âŒ **æ¨¡å‹ä¸å­˜åœ¨**: è·³è¿‡è¯¥æ¨¡å‹çš„æ‰€æœ‰æ¨ç†ä»»åŠ¡
- âŒ **LoRAé€‚é…å™¨ä¸å­˜åœ¨**: è·³è¿‡è¯¥ç‰¹å®šç»„åˆçš„æ¨ç†ä»»åŠ¡
- â­ï¸ **ç»“æœå·²å­˜åœ¨**: è·³è¿‡å·²å®Œæˆçš„æ¨ç†ä»»åŠ¡
- ğŸ“ **å¼‚å¸¸è®°å½•**: æ‰€æœ‰é”™è¯¯éƒ½ä¼šè®°å½•åœ¨æ—¥å¿—æ–‡ä»¶ä¸­

## æ³¨æ„äº‹é¡¹

1. **ä¾èµ–å…³ç³»**: ç¡®ä¿å·²å®‰è£… `vllm` å’Œç›¸å…³ä¾èµ–
2. **GPUå†…å­˜**: LoRAæ¨ç†ç›¸æ¯”å…¨å‚æ•°æ¨ç†èŠ‚çœå†…å­˜ï¼Œä½†ä»éœ€è¶³å¤Ÿçš„GPUå†…å­˜
3. **é€‚é…å™¨å…¼å®¹æ€§**: ç¡®ä¿LoRAé€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹ç‰ˆæœ¬å…¼å®¹
4. **ç½‘ç»œç¯å¢ƒ**: è„šæœ¬ä¼šè‡ªåŠ¨é…ç½®HuggingFaceé•œåƒï¼Œä½†é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹

## æµ‹è¯•éªŒè¯

ä½¿ç”¨æµ‹è¯•è„šæœ¬éªŒè¯é…ç½®ï¼š

```bash
python test_lora_inference.py
```

è¿™ä¼šæ˜¾ç¤ºå„ä¸ªæ¨¡å‹å’Œæ•°æ®é›†ç»„åˆçš„LoRAé€‚é…å™¨è·¯å¾„å’Œå­˜åœ¨çŠ¶æ€ã€‚ 