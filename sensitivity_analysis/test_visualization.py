#!/usr/bin/env python3
"""
å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•è„šæœ¬
Test Script for Visualization Functions

æµ‹è¯•æ•æ„Ÿæ€§åˆ†ææ¡†æ¶çš„å›¾è¡¨ç”ŸæˆåŠŸèƒ½
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å¯¼å…¥å¯è§†åŒ–å·¥å…·
from sensitivity_analysis.utils.plot_utils import SensitivityPlotter


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_dir = Path("sensitivity_analysis_test/logs")
    log_dir.mkdir(parents=True, exist_ok=True)

    log_file = log_dir / f"visualization_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )

    return logging.getLogger(__name__)


def create_mock_results():
    """åˆ›å»ºæ¨¡æ‹Ÿå®éªŒç»“æœç”¨äºæµ‹è¯•"""
    # æ•°æ®æ•æ„Ÿæ€§ç»“æœ
    data_sensitivity_results = []

    # æ¨¡æ‹Ÿå¤šä¸ªæ¨¡å‹çš„æ•°æ®æ•æ„Ÿæ€§ç»“æœ
    models = ['LLaMA-3.1-8B', 'ChatGLM3-6B', 'Qwen1.5-7B']
    data_sizes = [1000, 5000, 10000, 20000]

    for model_name in models:
        for data_size in data_sizes:
            # æ¨¡æ‹Ÿæ€§èƒ½æŒ‡æ ‡ï¼ˆæ•°æ®é‡è¶Šå¤§ï¼Œæ€§èƒ½è¶Šå¥½ï¼‰
            base_performance = 0.7 + 0.1 * (data_size / 20000)
            f1_score = base_performance + np.random.normal(0, 0.05)
            accuracy = min(1.0, base_performance + np.random.normal(0, 0.05))
            training_time = 60 + data_size * 0.01  # è®­ç»ƒæ—¶é—´éšæ•°æ®é‡å¢åŠ 
            memory_usage = 8 + data_size * 0.001  # å†…å­˜ä½¿ç”¨éšæ•°æ®é‡å¢åŠ 

            data_sensitivity_results.append({
                'experiment_id': f"exp_{model_name}_{data_size}",
                'model_name': model_name,
                'dataset_name': 'test_dataset',
                'data_size': data_size,
                'train_samples': int(data_size * 0.8),
                'test_samples': int(data_size * 0.2),
                'training_time': training_time,
                'inference_time': data_size * 0.001,
                'memory_usage': memory_usage,
                'status': 'completed',
                'timestamp': datetime.now().isoformat(),
                'accuracy': max(0, min(1.0, accuracy)),
                'f1_macro': max(0, min(1.0, f1_score)),
                'precision': max(0, min(1.0, f1_score + np.random.normal(0, 0.02))),
                'recall': max(0, min(1.0, f1_score + np.random.normal(0, 0.02))),
                'f1_micro': max(0, min(1.0, f1_score + np.random.normal(0, 0.02))),
                'f1_weighted': max(0, min(1.0, f1_score + np.random.normal(0, 0.02))),
                'auc': max(0, min(1.0, f1_score + np.random.normal(0, 0.03))),
                'inference_speed': data_size / (data_size * 0.001),
                'memory_efficiency': data_size / memory_usage,
                'training_efficiency': data_size / training_time
            })

    # LoRAå‚æ•°æ•æ„Ÿæ€§ç»“æœ
    lora_results = []
    lora_configs = [
        {'r': 8, 'alpha': 16, 'dropout': 0.0},
        {'r': 8, 'alpha': 32, 'dropout': 0.1},
        {'r': 16, 'alpha': 16, 'dropout': 0.0},
        {'r': 16, 'alpha': 32, 'dropout': 0.1},
        {'r': 32, 'alpha': 64, 'dropout': 0.05}
    ]

    for model_name in models[:2]:  # åªæµ‹è¯•å‰ä¸¤ä¸ªæ¨¡å‹çš„LoRAå‚æ•°
        for config in lora_configs:
            base_f1 = 0.8 + np.random.normal(0, 0.1)

            # LoRAå‚æ•°å½±å“æ¨¡æ‹Ÿ
            r_effect = config['r'] / 32  # rankçš„å½±å“
            alpha_effect = config['alpha'] / 64  # alphaçš„å½±å“
            dropout_penalty = config['dropout'] * 0.1  # dropoutçš„è´Ÿé¢å½±å“

            f1_score = max(0, min(1.0, base_f1 + r_effect * 0.1 + alpha_effect * 0.05 - dropout_penalty))

            lora_results.append({
                'experiment_id': f"exp_lora_{model_name}_{config['r']}",
                'model_name': model_name,
                'dataset_name': 'test_dataset',
                'lora_config': config,
                'data_size': 10000,
                'f1_macro': f1_score,
                'accuracy': f1_score + np.random.normal(0, 0.02),
                'training_time': 120,
                'status': 'completed',
                'timestamp': datetime.now().isoformat()
            })

    return data_sensitivity_results + lora_results


def test_visualization():
    """æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 50)
    logger.info("æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½...")
    logger.info("=" * 50)

    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "sensitivity_analysis_test/plots"
        os.makedirs(output_dir, exist_ok=True)

        # åˆ›å»ºå¯è§†åŒ–å™¨
        plotter = SensitivityPlotter(output_dir)

        # åˆ›å»ºæ¨¡æ‹Ÿç»“æœ
        logger.info("åˆ›å»ºæ¨¡æ‹Ÿå®éªŒç»“æœ...")
        results = create_mock_results()
        logger.info(f"åˆ›å»ºäº† {len(results)} ä¸ªæ¨¡æ‹Ÿå®éªŒç»“æœ")

        # ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
        logger.info("ç”Ÿæˆå›¾è¡¨...")
        plot_files = plotter.generate_all_plots(results)

        if plot_files:
            logger.info("âœ… å¯è§†åŒ–æµ‹è¯•æˆåŠŸï¼")
            logger.info("ç”Ÿæˆçš„å›¾è¡¨:")
            for plot_type, file_path in plot_files.items():
                logger.info(f"  - {plot_type}: {file_path}")

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path)
                    logger.info(f"    æ–‡ä»¶å¤§å°: {file_size} bytes")
                else:
                    logger.warning(f"    æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            return True
        else:
            logger.error("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•å›¾è¡¨æ–‡ä»¶")
            return False

    except Exception as e:
        logger.error(f"âŒ å¯è§†åŒ–æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    logger = setup_logging()

    logger.info("ğŸ¨ å¼€å§‹æ•æ„Ÿæ€§åˆ†æå¯è§†åŒ–åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 60)

    success = test_visualization()

    if success:
        logger.info("=" * 60)
        logger.info("ğŸ‰ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
        logger.info("\nğŸ“ æµ‹è¯•æ€»ç»“:")
        logger.info("- âœ… æ•æ„Ÿæ€§æ›²çº¿å›¾ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        logger.info("- âœ… æ€§èƒ½çƒ­åŠ›å›¾ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        logger.info("- âœ… å‚æ•°æ•æ„Ÿæ€§å›¾ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        logger.info("- âœ… æ•ˆç‡åˆ†æå›¾ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        logger.info("- âœ… ç»¼åˆHTMLæŠ¥å‘Šç”ŸæˆåŠŸèƒ½æ­£å¸¸")
        logger.info("\nğŸ–¼ï¸ ç”Ÿæˆçš„å›¾è¡¨æ ¼å¼ç¬¦åˆå‚è€ƒæ–‡æ¡£æ ‡å‡†:")
        logger.info("- main_sensitivity_analysis.png (ä¸»è¦æ•æ„Ÿæ€§åˆ†æå›¾)")
        logger.info("- performance_heatmap.png (æ€§èƒ½çƒ­åŠ›å›¾)")
        logger.info("- parameter_sensitivity_analysis.png (å‚æ•°æ•æ„Ÿæ€§å›¾)")
        logger.info("- efficiency_analysis.png (æ•ˆç‡åˆ†æå›¾)")
        logger.info("- comprehensive_analysis_report.html (ç»¼åˆæŠ¥å‘Š)")
        logger.info("\nğŸ’¡ ç°åœ¨æ¡†æ¶å®Œå…¨ç¬¦åˆå‚è€ƒæ–‡æ¡£çš„æ ¼å¼è¦æ±‚:")
        logger.info("- å›¾è¡¨ç±»å‹åŒ¹é… âœ…")
        logger.info("- è¾“å‡ºæ ¼å¼è§„èŒƒ âœ…")
        logger.info("- å‘½åæ ‡å‡†ä¸€è‡´ âœ…")
        logger.info("- æ•°æ®å±•ç¤ºå®Œæ•´ âœ…")
        logger.info("=" * 60)
    else:
        logger.error("âŒ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥")

    return success


if __name__ == "__main__":
    import numpy as np
    success = main()
    sys.exit(0 if success else 1)