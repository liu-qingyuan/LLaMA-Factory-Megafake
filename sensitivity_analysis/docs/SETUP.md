# ç¯å¢ƒè®¾ç½®æŒ‡å—

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### 1. åŸºç¡€ç¯å¢ƒè¦æ±‚

**ç¡¬ä»¶è¦æ±‚**:
- **GPU**: A100 40GB (æ¨è), RTX 4090 (æœ€ä½)
- **å†…å­˜**: 32GB+ (æ¨è), 16GB (æœ€ä½)
- **å­˜å‚¨**: 200GB+ SSD
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

**è½¯ä»¶è¦æ±‚**:
- Python 3.8+
- CUDA 11.8+
- Git

### 2. å®‰è£…æ­¥éª¤

#### æ­¥éª¤1: å…‹éš†é¡¹ç›®
```bash
git clone <repository-url>
cd LLaMA-Factory-Megafake
```

#### æ­¥éª¤2: å®‰è£…ä¾èµ–
```bash
# åŸºç¡€å®‰è£…
pip install -e ".[torch,metrics]" --no-build-isolation

# å®Œæ•´å®‰è£…ï¼ˆæ¨èï¼‰
pip install -e ".[torch,metrics,deepspeed,vllm,quantization]" --no-build-isolation
```

#### æ­¥éª¤3: é…ç½®ç¯å¢ƒå˜é‡
```bash
# HuggingFaceé•œåƒåŠ é€Ÿï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# ç½‘ç»œåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
source /etc/network_turbo

# GPUé€‰æ‹©
export CUDA_VISIBLE_DEVICES=0,1
```

### 3. æ¨¡å‹è®¾ç½®

#### æ”¯æŒçš„æ¨¡å‹
```bash
# æ¨¡å‹å­˜å‚¨ç›®å½•
mkdir -p /root/autodl-tmp/models

# æ¨èæ¨¡å‹åˆ—è¡¨
MODELS=(
    "Qwen1.5-7B"
    "Meta-Llama-3.1-8B-Instruct"
    "Baichuan2-7B-Chat"
    "Mistral-7B-v0.1"
    "chatglm3-6b"
)
```

#### æ¨¡å‹ä¸‹è½½è„šæœ¬ç¤ºä¾‹
```bash
#!/bin/bash
# download_models.sh

HF_MIRRORS=(
    "https://huggingface.co"
    "https://hf-mirror.com"
)

for model in "${MODELS[@]}"; do
    echo "ä¸‹è½½æ¨¡å‹: $model"
    # ä½¿ç”¨git lfsæˆ–å…¶ä»–ä¸‹è½½æ–¹æ³•
done
```

### 4. æ•°æ®é›†è®¾ç½®

#### æ•°æ®é›†ç›®å½•ç»“æ„
```
data/
â”œâ”€â”€ data_table/
â”‚   â”œâ”€â”€ task1/                          # å‡æ–°é—»æ£€æµ‹
â”‚   â”‚   â”œâ”€â”€ alpaca_full/
â”‚   â”‚   â”œâ”€â”€ small_8k/
â”‚   â”‚   â””â”€â”€ alpaca_test100_*/
â”‚   â”œâ”€â”€ task2/                          # ç»†ç²’åº¦åˆ†ç±»
â”‚   â””â”€â”€ task3/                          # å¤šæºéªŒè¯
â””â”€â”€ dataset_info.json                   # æ•°æ®é›†æ³¨å†Œè¡¨
```

#### æ•°æ®é›†é…ç½®
æ•°æ®é›†é…ç½®åœ¨ `data/dataset_info.json` ä¸­å®šä¹‰ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®é›†è·¯å¾„æ­£ç¡®ã€‚

### 5. éªŒè¯å®‰è£…

#### å¿«é€Ÿæµ‹è¯•
```bash
# éªŒè¯åŸºç¡€åŠŸèƒ½
python scripts/sa.py test

# éªŒè¯æ¨¡å‹åŠ è½½
python sensitivity_analysis/model_utils/verify_models.py

# éªŒè¯VLLMä¿®å¤
python sensitivity_analysis/scripts/test_vllm_fix.py
```

#### åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œå¿«é€Ÿæ•æ„Ÿæ€§åˆ†æ
python scripts/sa.py quick

# æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
python scripts/sa.py monitor
```

## ğŸ”§ é«˜çº§é…ç½®

### 1. VLLMåŠ é€Ÿé…ç½®

```bash
# å¯ç”¨VLLM
VLLM_ENABLED=true python scripts/multi_model_inference.py

# VLLMé…ç½®ç¤ºä¾‹
python scripts/vllm_infer.py \
  --model_name_or_path /root/autodl-tmp/models/Qwen1.5-7B \
  --template qwen \
  --dataset task1_small_glm \
  --vllm_config '{"tensor_parallel_size": 1}'
```

### 2. å†…å­˜ä¼˜åŒ–

```python
# å†…å­˜ä¼˜åŒ–æ¨¡å¼
python sensitivity_analysis/scripts/run_analysis.py \
  --mode quick \
  --memory-optimized
```

### 3. å¤šGPUé…ç½®

```bash
# åˆ†å¸ƒå¼è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 llamafactory-cli train config.yaml

# æ•°æ®å¹¶è¡Œ
export NCCL_DEBUG=INFO
```

### 4. æ€§èƒ½è°ƒä¼˜

#### æ¨èå‚æ•°
```yaml
# LoRAå‚æ•°
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# è®­ç»ƒå‚æ•°
learning_rate: 2e-5
batch_size: 8
gradient_accumulation_steps: 8

# ä¼˜åŒ–å‚æ•°
flash_attn: "auto"
fp16: true
ddp_timeout: 180000000
```

## ğŸ› å¸¸è§é—®é¢˜è§£å†³

### 1. å¯¼å…¥é”™è¯¯

**é—®é¢˜**: `ModuleNotFoundError: No module named 'xxx'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /root/autodl-tmp/LLaMA-Factory-Megafake

# é‡æ–°å®‰è£…
pip install -e ".[torch,metrics,deepspeed,vllm,quantization]" --no-build-isolation
```

### 2. CUDAé”™è¯¯

**é—®é¢˜**: `CUDA error: device-side assert triggered`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
export BATCH_SIZE=4

# ä½¿ç”¨CPUä½œä¸ºå¤‡é€‰
export CUDA_VISIBLE_DEVICES=""

# æ£€æŸ¥GPUå†…å­˜
nvidia-smi
```

### 3. å†…å­˜ä¸è¶³

**é—®é¢˜**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
export GRADIENT_CHECKPOINTING=true

# å‡å°‘æ¨¡å‹å¹¶è¡Œ
export TENSOR_PARALLEL_SIZE=1

# ä½¿ç”¨æ··åˆç²¾åº¦
export FP16=true
```

### 4. ç½‘ç»œé—®é¢˜

**é—®é¢˜**: ä¸‹è½½æ¨¡å‹/æ•°æ®é›†å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# è®¾ç½®ä»£ç†
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# å¢åŠ é‡è¯•æ¬¡æ•°
export HF_HUB_OFFLINE=false
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### ç¡¬ä»¶æ€§èƒ½å‚è€ƒ

| GPUå‹å· | æ¨ç†é€Ÿåº¦ | è®­ç»ƒé€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | æ¨èç”¨é€” |
|---------|----------|----------|----------|----------|
| A100 40GB | 3x | 1x | 100% | ç”Ÿäº§ç¯å¢ƒ |
| RTX 4090 | 2x | 0.8x | 80% | å¼€å‘æµ‹è¯• |
| RTX 3090 | 1.5x | 0.6x | 60% | å°è§„æ¨¡å®éªŒ |

### æ¨èé…ç½®

#### å¿«é€Ÿå¼€å‘
- GPU: RTX 3090
- æ¨¡å‹: Qwen1.5-7B
- æ•°æ®: 1K-5Kæ ·æœ¬
- LoRA: r=16, alpha=32

#### ç”Ÿäº§ç¯å¢ƒ
- GPU: A100 40GB
- æ¨¡å‹: LLaMA-3.1-8B
- æ•°æ®: 10K-50K+æ ·æœ¬
- LoRA: r=32, alpha=64

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-17
**ç»´æŠ¤çŠ¶æ€**: âœ… æ´»è·ƒç»´æŠ¤ä¸­