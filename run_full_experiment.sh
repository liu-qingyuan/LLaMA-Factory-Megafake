#!/bin/bash

# -----------------------------------------------------------------------------
# å…¨é‡è§„æ¨¡å®éªŒè‡ªåŠ¨è¿è¡Œè„šæœ¬
# åŠŸèƒ½: ä¾æ¬¡æ‰§è¡Œ 1k -> 2k -> 5k -> 10k -> 20k çš„ è®­ç»ƒ -> æ¨ç† -> åˆ†æ å…¨æµç¨‹
# -----------------------------------------------------------------------------

# å®šä¹‰ç¯å¢ƒå˜é‡
export HF_ENDPOINT=https://hf-mirror.com
export HF_HOME=.cache/huggingface

# æ—¥å¿—æ–‡ä»¶
LOG_FILE="sensitivity_analysis/logs/full_experiment_$(date +%Y%m%d_%H%M%S).log"
mkdir -p sensitivity_analysis/logs

echo "========================================================" | tee -a "$LOG_FILE"
echo "ğŸš€ å¼€å§‹å¤§è§„æ¨¡å…¨é‡å®éªŒ (Start Time: $(date))" | tee -a "$LOG_FILE"
echo "ğŸ“œ è¯¦ç»†æ—¥å¿—å°†å†™å…¥: $LOG_FILE" | tee -a "$LOG_FILE"
echo "========================================================" | tee -a "$LOG_FILE"

# å®šä¹‰æ¨¡å‹åˆ—è¡¨
MODELS="Meta-Llama-3.1-8B-Instruct Qwen1.5-7B chatglm3-6b Mistral-7B-v0.1 Baichuan2-7B-Chat"

# å®šä¹‰æ•°æ®é›†è§„æ¨¡åˆ—è¡¨
SCALES=("task1_scale_1000_glm" "task1_scale_2000_glm" "task1_scale_5000_glm" "task1_scale_10000_glm" "task1_scale_20000_glm")

# å¾ªç¯å¤„ç†æ¯ä¸ªè§„æ¨¡
for dataset in "${SCALES[@]}"; do
    start_time=$(date +%s)
    echo "" | tee -a "$LOG_FILE"
    echo "########################################################" | tee -a "$LOG_FILE"
    echo "ğŸ“¦ æ­£åœ¨å¤„ç†æ•°æ®é›†è§„æ¨¡: $dataset" | tee -a "$LOG_FILE"
    echo "########################################################" | tee -a "$LOG_FILE"

    # 1. è®­ç»ƒ (Training)
    echo "ğŸ‘‰ [1/3] å¼€å§‹è®­ç»ƒ (Training)..." | tee -a "$LOG_FILE"
    python scripts/multi_model_lora_train.py --models $MODELS --datasets $dataset >> "$LOG_FILE" 2>&1
    if [ $? -eq 0 ]; then
        echo "âœ… è®­ç»ƒå‘½ä»¤æ‰§è¡Œå®Œæ¯•" | tee -a "$LOG_FILE"
    else
        echo "âŒ è®­ç»ƒå‘½ä»¤æ‰§è¡Œå‡ºé”™ (ä½†å°†å°è¯•ç»§ç»­æ¨ç†)" | tee -a "$LOG_FILE"
    fi

    # 2. æ¨ç† (Inference)
    echo "ğŸ‘‰ [2/3] å¼€å§‹æ¨ç† (Inference)..." | tee -a "$LOG_FILE"
    python scripts/multi_model_lora_inference.py --models $MODELS --datasets $dataset >> "$LOG_FILE" 2>&1
    
    # 3. åˆ†æ (Analysis)
    echo "ğŸ‘‰ [3/3] å¼€å§‹åˆ†æä¸ç»˜å›¾ (Analysis)..." | tee -a "$LOG_FILE"
    
    # æå–è§„æ¨¡æ•°å€¼ (ä¾‹å¦‚ 1000)
    scale_val=$(echo $dataset | sed -n 's/.*scale_\([0-9]*\)_glm/\1/p')
    
    result_dir="sensitivity_analysis/outputs/task1/scale_${scale_val}"
    plot_dir="sensitivity_analysis/results/plots/scale_${scale_val}"
    csv_file="sensitivity_analysis/results/scale_${scale_val}_metrics.csv"
    
    if [ -d "$result_dir" ]; then
        python scripts/analyze_predictions.py \
            --dir "$result_dir" \
            --output "$csv_file" \
            --plot --plot-dir "$plot_dir" >> "$LOG_FILE" 2>&1
        
        if [ -f "$csv_file" ]; then
            echo "ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: $csv_file" | tee -a "$LOG_FILE"
        else
            echo "âš ï¸  åˆ†æè„šæœ¬è¿è¡Œå®Œæˆä½†æœªç”Ÿæˆ CSV" | tee -a "$LOG_FILE"
        fi
    else
        echo "âš ï¸  æœªæ‰¾åˆ°ç»“æœç›®å½•ï¼Œè·³è¿‡åˆ†æ: $result_dir" | tee -a "$LOG_FILE"
    fi

    end_time=$(date +%s)
    duration=$((end_time - start_time))
    echo "â±ï¸  è§„æ¨¡ $dataset å¤„ç†è€—æ—¶: ${duration} ç§’" | tee -a "$LOG_FILE"
done

echo "" | tee -a "$LOG_FILE"
echo "========================================================" | tee -a "$LOG_FILE"
echo "ğŸ‰ æ‰€æœ‰è§„æ¨¡å®éªŒå·²å®Œæˆ! (End Time: $(date))" | tee -a "$LOG_FILE"
echo "========================================================" | tee -a "$LOG_FILE"
